
Data preprocessing...
TESTING FOR MUTLIPLE PARAMETERS:




====================================================================================================
Training model: bert-base-multilingual-cased
 * 10 epochs
 * learning rate is 2e-05
 * train language: de
 * test language: en,it
 * dropout: 0.3
 * batch size is 16
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50
____________________________________________________________________________________________________

Epoch 1/10, accuracy: 0.1583, loss: 1.7246
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 2/10, accuracy: 0.2639, loss: 1.2688
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 3/10, accuracy: 0.3371, loss: 1.1305
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 4/10, accuracy: 0.3716, loss: 1.0485
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 5/10, accuracy: 0.3889, loss: 0.9818
 * Micro Average: f1: 0.0002, precision: 0.0001, recall: 0.0002
 * Macro Average: f1: 0.0002, precision: 0.0004, recall: 0.0001

Epoch 6/10, accuracy: 0.4081, loss: 0.9311
 * Micro Average: f1: 0.0006, precision: 0.0005, recall: 0.0007
 * Macro Average: f1: 0.0006, precision: 0.0009, recall: 0.0004

Epoch 7/10, accuracy: 0.4211, loss: 0.9037
 * Micro Average: f1: 0.0016, precision: 0.0014, recall: 0.0020
 * Macro Average: f1: 0.0015, precision: 0.0018, recall: 0.0012

Epoch 8/10, accuracy: 0.4313, loss: 0.8793
 * Micro Average: f1: 0.0036, precision: 0.0030, recall: 0.0046
 * Macro Average: f1: 0.0032, precision: 0.0069, recall: 0.0028

Epoch 9/10, accuracy: 0.4397, loss: 0.8707
 * Micro Average: f1: 0.0056, precision: 0.0046, recall: 0.0071
 * Macro Average: f1: 0.0048, precision: 0.0080, recall: 0.0044

Epoch 10/10, accuracy: 0.4421, loss: 0.8589
 * Micro Average: f1: 0.0069, precision: 0.0057, recall: 0.0088
 * Macro Average: f1: 0.0058, precision: 0.0087, recall: 0.0054

Classification Report on test set:

________________________________________
 CLS]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 LOC:
  * precision: 0.0076
  * recall: 0.0004
  * f1-score: 0.0008
  * support: 4825.0000
 ORG:
  * precision: 0.0125
  * recall: 0.0019
  * f1-score: 0.0033
  * support: 4666.0000
 PAD]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 PER:
  * precision: 0.0212
  * recall: 0.0190
  * f1-score: 0.0200
  * support: 4630.0000
 SEP]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 micro avg:
  * precision: 0.0047
  * recall: 0.0070
  * f1-score: 0.0056
  * support: 14121.0000
 macro avg:
  * precision: 0.0069
  * recall: 0.0036
  * f1-score: 0.0040
  * support: 14121.0000
 weighted avg:
  * precision: 0.0137
  * recall: 0.0070
  * f1-score: 0.0079
  * support: 14121.0000
 accuracy:
  * 0.4430
________________________________________


Saving model to .
Model saved!

----------------------------------------------------------------------------------------------------



====================================================================================================
BEST MODEL (f1: 0.005594):
Training model: bert-base-multilingual-cased
 * 10 epochs
 * learning rate is 2e-05
 * train language: de
 * test language: en,it
 * dropout: 0.3
 * batch size is 16
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50

Saving model to .
Model saved!

----------------------------------------------------------------------------------------------------


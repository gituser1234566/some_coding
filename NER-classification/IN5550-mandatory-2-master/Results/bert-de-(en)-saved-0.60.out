====================================================================================================
Training model: bert-base-multilingual-cased
 * 5 epochs
 * learning rate is 2e-05
 * train language: de
 * test language: en
 * dropout: 0.2
 * batch size is 16
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50
____________________________________________________________________________________________________

/itf-fi-ml/home/eirikeg/.local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [SEP] seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/itf-fi-ml/home/eirikeg/.local/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Epoch 1/5, accuracy: 0.8219, loss: 0.2112
 * Micro Average: f1: 0.5720, precision: 0.5257, recall: 0.6272
 * Macro Average: f1: 0.4330, precision: 0.4039, recall: 0.4707

Epoch 2/5, accuracy: 0.8150, loss: 0.0699
 * Micro Average: f1: 0.5758, precision: 0.5372, recall: 0.6203
 * Macro Average: f1: 0.5794, precision: 0.5464, recall: 0.6209

Epoch 3/5, accuracy: 0.8227, loss: 0.0515
 * Micro Average: f1: 0.5925, precision: 0.5506, recall: 0.6414
 * Macro Average: f1: 0.5958, precision: 0.5587, recall: 0.6420

/itf-fi-ml/home/eirikeg/.local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [PAD] seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
Epoch 4/5, accuracy: 0.8205, loss: 0.0418
 * Micro Average: f1: 0.5909, precision: 0.5503, recall: 0.6380
 * Macro Average: f1: 0.4453, precision: 0.4187, recall: 0.4790

Epoch 5/5, accuracy: 0.8175, loss: 0.0349
 * Micro Average: f1: 0.5934, precision: 0.5561, recall: 0.6361
 * Macro Average: f1: 0.5961, precision: 0.5636, recall: 0.6368

Classification Report on test set:

________________________________________
 LOC:
  * precision: 0.4802
  * recall: 0.6095
  * f1-score: 0.5372
  * support: 4825.0000
 ORG:
  * precision: 0.6149
  * recall: 0.6097
  * f1-score: 0.6123
  * support: 4666.0000
 PAD]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 PER:
  * precision: 0.6392
  * recall: 0.7199
  * f1-score: 0.6772
  * support: 4630.0000
 micro avg:
  * precision: 0.5712
  * recall: 0.6458
  * f1-score: 0.6062
  * support: 14121.0000
 macro avg:
  * precision: 0.4336
  * recall: 0.4848
  * f1-score: 0.4567
  * support: 14121.0000
 weighted avg:
  * precision: 0.5769
  * recall: 0.6458
  * f1-score: 0.6079
  * support: 14121.0000
 accuracy:
  * 0.8183
________________________________________


Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Saving model to .
Model saved!

----------------------------------------------------------------------------------------------------



====================================================================================================
BEST MODEL (f1: 0.6062):
Training model: bert-base-multilingual-cased
 * 5 epochs
 * learning rate is 2e-05
 * train language: de
 * test language: en
 * dropout: 0.2
 * batch size is 16
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50

Saving model to .
Model saved!

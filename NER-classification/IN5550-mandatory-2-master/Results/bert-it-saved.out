
Data preprocessing...
TESTING FOR MUTLIPLE PARAMETERS:




====================================================================================================
Training model: bert-base-multilingual-cased
 * 5 epochs
 * learning rate is 2e-05
 * train language: it
 * test language: en
 * dropout: 0.2
 * batch size is 16
 * frozen weights: False
 * activation: GELU
 * device is on cuda
 * warmup steps: 50
____________________________________________________________________________________________________

Epoch 1/5, accuracy: 0.8559, loss: 0.2196
 * Micro Average: f1: 0.6309, precision: 0.5764, recall: 0.6969
 * Macro Average: f1: 0.6408, precision: 0.5974, recall: 0.6979

Epoch 2/5, accuracy: 0.8645, loss: 0.0682
 * Micro Average: f1: 0.6607, precision: 0.6110, recall: 0.7191
 * Macro Average: f1: 0.6699, precision: 0.6311, recall: 0.7200

Epoch 3/5, accuracy: 0.8680, loss: 0.0504
 * Micro Average: f1: 0.6681, precision: 0.6209, recall: 0.7231
 * Macro Average: f1: 0.6735, precision: 0.6310, recall: 0.7245

Epoch 4/5, accuracy: 0.8682, loss: 0.0407
 * Micro Average: f1: 0.6741, precision: 0.6270, recall: 0.7290
 * Macro Average: f1: 0.6807, precision: 0.6400, recall: 0.7305

Epoch 5/5, accuracy: 0.8713, loss: 0.0338
 * Micro Average: f1: 0.6774, precision: 0.6306, recall: 0.7317
 * Macro Average: f1: 0.6845, precision: 0.6451, recall: 0.7333

Classification Report on test set:

________________________________________
 LOC:
  * precision: 0.5198
  * recall: 0.6889
  * f1-score: 0.5925
  * support: 4825.0000
 ORG:
  * precision: 0.6070
  * recall: 0.6727
  * f1-score: 0.6382
  * support: 4666.0000
 PER:
  * precision: 0.8342
  * recall: 0.8616
  * f1-score: 0.8476
  * support: 4630.0000
 micro avg:
  * precision: 0.6393
  * recall: 0.7402
  * f1-score: 0.6861
  * support: 14121.0000
 macro avg:
  * precision: 0.6537
  * recall: 0.7411
  * f1-score: 0.6928
  * support: 14121.0000
 weighted avg:
  * precision: 0.6517
  * recall: 0.7402
  * f1-score: 0.6913
  * support: 14121.0000
 accuracy:
  * 0.8706
________________________________________


Saving model to .
Model saved!

----------------------------------------------------------------------------------------------------



====================================================================================================
BEST MODEL (f1: 0.6861):
Training model: bert-base-multilingual-cased
 * 5 epochs
 * learning rate is 2e-05
 * train language: it
 * test language: en
 * dropout: 0.2
 * batch size is 16
 * frozen weights: False
 * activation: GELU
 * device is on cuda
 * warmup steps: 50

Saving model to .
Model saved!

----------------------------------------------------------------------------------------------------


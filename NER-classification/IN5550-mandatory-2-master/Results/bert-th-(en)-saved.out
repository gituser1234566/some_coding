Data preprocessing...
TESTING FOR MUTLIPLE PARAMETERS:


Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


====================================================================================================
Training model: bert-base-multilingual-cased
 * 5 epochs
 * learning rate is 2e-05
 * train language: th
 * test language: en
 * dropout: 0.2
 * batch size is 16
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50
____________________________________________________________________________________________________

/itf-fi-ml/home/eirikeg/.local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [SEP] seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/itf-fi-ml/home/eirikeg/.local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [PAD] seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/itf-fi-ml/home/eirikeg/.local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [CLS] seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/itf-fi-ml/home/eirikeg/.local/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Epoch 1/5, accuracy: 0.4364, loss: 0.2261
 * Micro Average: f1: 0.0404, precision: 0.0289, recall: 0.0671
 * Macro Average: f1: 0.0193, precision: 0.0139, recall: 0.0336

Epoch 2/5, accuracy: 0.4938, loss: 0.1111
 * Micro Average: f1: 0.0911, precision: 0.0727, recall: 0.1218
 * Macro Average: f1: 0.0344, precision: 0.0245, recall: 0.0610

Epoch 3/5, accuracy: 0.4675, loss: 0.0913
 * Micro Average: f1: 0.1120, precision: 0.0921, recall: 0.1428
 * Macro Average: f1: 0.0386, precision: 0.0275, recall: 0.0718

Epoch 4/5, accuracy: 0.5158, loss: 0.0792
 * Micro Average: f1: 0.1153, precision: 0.0924, recall: 0.1532
 * Macro Average: f1: 0.0471, precision: 0.0408, recall: 0.0761

Epoch 5/5, accuracy: 0.5026, loss: 0.0769
 * Micro Average: f1: 0.1164, precision: 0.0924, recall: 0.1570
 * Macro Average: f1: 0.0476, precision: 0.0444, recall: 0.0781

Classification Report on test set:

________________________________________
 CLS]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 LOC:
  * precision: 0.1278
  * recall: 0.2922
  * f1-score: 0.1779
  * support: 4825.0000
 ORG:
  * precision: 0.0718
  * recall: 0.1556
  * f1-score: 0.0983
  * support: 4666.0000
 PAD]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 PER:
  * precision: 0.0337
  * recall: 0.0013
  * f1-score: 0.0025
  * support: 4630.0000
 SEP]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 micro avg:
  * precision: 0.0908
  * recall: 0.1517
  * f1-score: 0.1136
  * support: 14121.0000
 macro avg:
  * precision: 0.0389
  * recall: 0.0749
  * f1-score: 0.0464
  * support: 14121.0000
 weighted avg:
  * precision: 0.0785
  * recall: 0.1517
  * f1-score: 0.0941
  * support: 14121.0000
 accuracy:
  * 0.4925
________________________________________


Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Saving model to .
Model saved!

----------------------------------------------------------------------------------------------------



====================================================================================================
BEST MODEL (f1: 0.1136):
Training model: bert-base-multilingual-cased
 * 5 epochs
 * learning rate is 2e-05
 * train language: th
 * test language: en
 * dropout: 0.2
 * batch size is 16
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50

Saving model to .
Model saved!

----------------------------------------------------------------------------------------------------

bash-4.4$ 
Starting job 460142 on gpu-9 at Wed Mar 13 10:56:28 CET 2024

submission directory: /fp/homes01/u01/ec-eirikeg/mandatory_2
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/fp/projects01/ec30/software/easybuild/software/nlpl-nlptools/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [PAD] seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/fp/projects01/ec30/software/easybuild/software/nlpl-nlptools/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/projects01/ec30/software/easybuild/software/nlpl-nlptools/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/fp/projects01/ec30/software/easybuild/software/nlpl-nlptools/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [SEP] seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/fp/projects01/ec30/software/easybuild/software/nlpl-nlptools/01-foss-2022b-Python-3.10.8/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [CLS] seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Data preprocessing...
TESTING FOR MUTLIPLE PARAMETERS:




====================================================================================================
Training model: bert-base-multilingual-cased
 * 10 epochs
 * learning rate is 2e-05
 * train language: en-it
 * test language: en,it,de,sw,af,ru
 * dropout: 0.3
 * batch size is 32
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50
____________________________________________________________________________________________________

Epoch 1/10, accuracy: 0.1189, loss: 1.8943
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 2/10, accuracy: 0.1601, loss: 1.3986
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 3/10, accuracy: 0.2356, loss: 1.2422
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 4/10, accuracy: 0.2997, loss: 1.1590
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 5/10, accuracy: 0.3344, loss: 1.1048
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 6/10, accuracy: 0.3606, loss: 1.0597
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 7/10, accuracy: 0.3780, loss: 1.0285
 * Micro Average: f1: 0.0002, precision: 0.0001, recall: 0.0002
 * Macro Average: f1: 0.0003, precision: 0.0021, recall: 0.0001

Epoch 8/10, accuracy: 0.3914, loss: 1.0092
 * Micro Average: f1: 0.0002, precision: 0.0001, recall: 0.0002
 * Macro Average: f1: 0.0002, precision: 0.0012, recall: 0.0001

Epoch 9/10, accuracy: 0.3996, loss: 0.9945
 * Micro Average: f1: 0.0002, precision: 0.0001, recall: 0.0002
 * Macro Average: f1: 0.0002, precision: 0.0010, recall: 0.0001

Epoch 10/10, accuracy: 0.4024, loss: 0.9922
 * Micro Average: f1: 0.0006, precision: 0.0005, recall: 0.0007
 * Macro Average: f1: 0.0008, precision: 0.0032, recall: 0.0004

Finished training in 177.96 seconds

Classification Report on test set:

________________________________________
 LOC:
  * precision: 0.0035
  * recall: 0.0004
  * f1-score: 0.0007
  * support: 4825.0000
 ORG:
  * precision: 0.0044
  * recall: 0.0009
  * f1-score: 0.0014
  * support: 4666.0000
 PAD]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 PER:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 4630.0000
 SEP]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 micro avg:
  * precision: 0.0003
  * recall: 0.0004
  * f1-score: 0.0004
  * support: 14121.0000
 macro avg:
  * precision: 0.0016
  * recall: 0.0003
  * f1-score: 0.0004
  * support: 14121.0000
 weighted avg:
  * precision: 0.0027
  * recall: 0.0004
  * f1-score: 0.0007
  * support: 14121.0000
 accuracy:
  * 0.4016
________________________________________


Saving model to /fp/projects01/ec30/eirikeg/models
Model saved!

----------------------------------------------------------------------------------------------------



====================================================================================================
Training model: bert-base-multilingual-cased
 * 10 epochs
 * learning rate is 2e-05
 * train language: en-it
 * test language: en,it,de,sw,af,ru
 * dropout: 0.3
 * batch size is 32
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50
____________________________________________________________________________________________________

Epoch 1/10, accuracy: 0.1140, loss: 1.9092
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 2/10, accuracy: 0.1713, loss: 1.4142
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 3/10, accuracy: 0.3025, loss: 1.2539
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 4/10, accuracy: 0.3903, loss: 1.1818
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 5/10, accuracy: 0.4315, loss: 1.1269
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 6/10, accuracy: 0.4546, loss: 1.0731
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 7/10, accuracy: 0.4730, loss: 1.0460
 * Micro Average: f1: 0.0002, precision: 0.0001, recall: 0.0002
 * Macro Average: f1: 0.0002, precision: 0.0008, recall: 0.0001

Epoch 8/10, accuracy: 0.4838, loss: 1.0144
 * Micro Average: f1: 0.0008, precision: 0.0007, recall: 0.0009
 * Macro Average: f1: 0.0009, precision: 0.0029, recall: 0.0005

Epoch 9/10, accuracy: 0.4927, loss: 1.0052
 * Micro Average: f1: 0.0010, precision: 0.0009, recall: 0.0011
 * Macro Average: f1: 0.0010, precision: 0.0029, recall: 0.0006

Epoch 10/10, accuracy: 0.4952, loss: 0.9958
 * Micro Average: f1: 0.0010, precision: 0.0009, recall: 0.0011
 * Macro Average: f1: 0.0010, precision: 0.0029, recall: 0.0006

Finished training in 180.09 seconds

Classification Report on test set:

________________________________________
 CLS]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 LOC:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 4595.0000
 ORG:
  * precision: 0.0021
  * recall: 0.0002
  * f1-score: 0.0004
  * support: 4108.0000
 PAD]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 PER:
  * precision: 0.0065
  * recall: 0.0014
  * f1-score: 0.0023
  * support: 4906.0000
 SEP]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 micro avg:
  * precision: 0.0005
  * recall: 0.0006
  * f1-score: 0.0005
  * support: 13609.0000
 macro avg:
  * precision: 0.0014
  * recall: 0.0003
  * f1-score: 0.0005
  * support: 13609.0000
 weighted avg:
  * precision: 0.0030
  * recall: 0.0006
  * f1-score: 0.0010
  * support: 13609.0000
 accuracy:
  * 0.4890
________________________________________


Saving model to /fp/projects01/ec30/eirikeg/models
Model saved!

----------------------------------------------------------------------------------------------------



====================================================================================================
Training model: bert-base-multilingual-cased
 * 10 epochs
 * learning rate is 2e-05
 * train language: en-it
 * test language: en,it,de,sw,af,ru
 * dropout: 0.3
 * batch size is 32
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50
____________________________________________________________________________________________________

Epoch 1/10, accuracy: 0.0974, loss: 1.9010
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 2/10, accuracy: 0.1207, loss: 1.3996
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 3/10, accuracy: 0.2369, loss: 1.2431
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 4/10, accuracy: 0.3485, loss: 1.1678
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 5/10, accuracy: 0.4237, loss: 1.1030
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 6/10, accuracy: 0.4724, loss: 1.0674
 * Micro Average: f1: 0.0001, precision: 0.0001, recall: 0.0002
 * Macro Average: f1: 0.0002, precision: 0.0012, recall: 0.0001

Epoch 7/10, accuracy: 0.5035, loss: 1.0315
 * Micro Average: f1: 0.0003, precision: 0.0002, recall: 0.0004
 * Macro Average: f1: 0.0003, precision: 0.0012, recall: 0.0002

Epoch 8/10, accuracy: 0.5262, loss: 1.0140
 * Micro Average: f1: 0.0008, precision: 0.0006, recall: 0.0011
 * Macro Average: f1: 0.0009, precision: 0.0020, recall: 0.0006

Epoch 9/10, accuracy: 0.5360, loss: 0.9924
 * Micro Average: f1: 0.0012, precision: 0.0009, recall: 0.0017
 * Macro Average: f1: 0.0013, precision: 0.0034, recall: 0.0008

Epoch 10/10, accuracy: 0.5394, loss: 0.9829
 * Micro Average: f1: 0.0014, precision: 0.0011, recall: 0.0020
 * Macro Average: f1: 0.0016, precision: 0.0037, recall: 0.0010

Finished training in 187.67 seconds

Classification Report on test set:

________________________________________
 CLS]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 LOC:
  * precision: 0.0155
  * recall: 0.0012
  * f1-score: 0.0022
  * support: 4961.0000
 ORG:
  * precision: 0.0129
  * recall: 0.0049
  * f1-score: 0.0071
  * support: 4273.0000
 PAD]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 PER:
  * precision: 0.0026
  * recall: 0.0007
  * f1-score: 0.0011
  * support: 4565.0000
 SEP]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 micro avg:
  * precision: 0.0012
  * recall: 0.0022
  * f1-score: 0.0016
  * support: 13799.0000
 macro avg:
  * precision: 0.0052
  * recall: 0.0011
  * f1-score: 0.0017
  * support: 13799.0000
 weighted avg:
  * precision: 0.0104
  * recall: 0.0022
  * f1-score: 0.0034
  * support: 13799.0000
 accuracy:
  * 0.5444
________________________________________


Saving model to /fp/projects01/ec30/eirikeg/models
Model saved!

----------------------------------------------------------------------------------------------------



====================================================================================================
Training model: bert-base-multilingual-cased
 * 10 epochs
 * learning rate is 2e-05
 * train language: en-it
 * test language: en,it,de,sw,af,ru
 * dropout: 0.3
 * batch size is 32
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50
____________________________________________________________________________________________________

Epoch 1/10, accuracy: 0.0381, loss: 1.9030
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 2/10, accuracy: 0.0558, loss: 1.4093
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 3/10, accuracy: 0.1097, loss: 1.2511
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 4/10, accuracy: 0.1614, loss: 1.1638
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 5/10, accuracy: 0.2042, loss: 1.1130
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 6/10, accuracy: 0.2265, loss: 1.0716
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 7/10, accuracy: 0.2394, loss: 1.0370
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 8/10, accuracy: 0.2523, loss: 1.0123
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 9/10, accuracy: 0.2617, loss: 0.9915
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 10/10, accuracy: 0.2635, loss: 0.9936
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Finished training in 123.57 seconds

Classification Report on test set:

________________________________________
 LOC:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 446.0000
 ORG:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 351.0000
 PAD]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 PER:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 402.0000
 micro avg:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 1199.0000
 macro avg:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 1199.0000
 weighted avg:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 1199.0000
 accuracy:
  * 0.2675
________________________________________


Saving model to /fp/projects01/ec30/eirikeg/models
Model saved!

----------------------------------------------------------------------------------------------------



====================================================================================================
Training model: bert-base-multilingual-cased
 * 10 epochs
 * learning rate is 2e-05
 * train language: en-it
 * test language: en,it,de,sw,af,ru
 * dropout: 0.3
 * batch size is 32
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50
____________________________________________________________________________________________________

Epoch 1/10, accuracy: 0.0691, loss: 1.8935
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 2/10, accuracy: 0.0811, loss: 1.4051
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 3/10, accuracy: 0.1844, loss: 1.2535
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 4/10, accuracy: 0.3079, loss: 1.1782
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 5/10, accuracy: 0.4022, loss: 1.1187
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 6/10, accuracy: 0.4504, loss: 1.0705
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 7/10, accuracy: 0.4917, loss: 1.0429
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 8/10, accuracy: 0.5119, loss: 1.0193
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 9/10, accuracy: 0.5224, loss: 1.0130
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 10/10, accuracy: 0.5252, loss: 0.9991
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Finished training in 141.52 seconds

Classification Report on test set:

________________________________________
 CLS]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 LOC:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 526.0000
 ORG:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 581.0000
 PAD]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 PER:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 365.0000
 SEP]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 micro avg:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 1472.0000
 macro avg:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 1472.0000
 weighted avg:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 1472.0000
 accuracy:
  * 0.5347
________________________________________


Saving model to /fp/projects01/ec30/eirikeg/models
Model saved!

----------------------------------------------------------------------------------------------------



====================================================================================================
Training model: bert-base-multilingual-cased
 * 10 epochs
 * learning rate is 2e-05
 * train language: en-it
 * test language: en,it,de,sw,af,ru
 * dropout: 0.3
 * batch size is 32
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50
____________________________________________________________________________________________________

Epoch 1/10, accuracy: 0.1233, loss: 1.9208
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 2/10, accuracy: 0.1473, loss: 1.4176
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 3/10, accuracy: 0.2217, loss: 1.2575
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 4/10, accuracy: 0.2970, loss: 1.1729
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 5/10, accuracy: 0.3425, loss: 1.1149
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 6/10, accuracy: 0.3730, loss: 1.0750
 * Micro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000
 * Macro Average: f1: 0.0000, precision: 0.0000, recall: 0.0000

Epoch 7/10, accuracy: 0.3929, loss: 1.0432
 * Micro Average: f1: 0.0002, precision: 0.0001, recall: 0.0002
 * Macro Average: f1: 0.0002, precision: 0.0006, recall: 0.0001

Epoch 8/10, accuracy: 0.4061, loss: 1.0198
 * Micro Average: f1: 0.0002, precision: 0.0001, recall: 0.0002
 * Macro Average: f1: 0.0002, precision: 0.0004, recall: 0.0001

Epoch 9/10, accuracy: 0.4131, loss: 1.0091
 * Micro Average: f1: 0.0002, precision: 0.0001, recall: 0.0002
 * Macro Average: f1: 0.0002, precision: 0.0004, recall: 0.0001

Epoch 10/10, accuracy: 0.4160, loss: 0.9975
 * Micro Average: f1: 0.0002, precision: 0.0001, recall: 0.0002
 * Macro Average: f1: 0.0002, precision: 0.0004, recall: 0.0001

Finished training in 189.66 seconds

Classification Report on test set:

________________________________________
 CLS]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 LOC:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 4845.0000
 ORG:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 3887.0000
 PAD]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 PER:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 3583.0000
 SEP]:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 0.0000
 micro avg:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 12315.0000
 macro avg:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 12315.0000
 weighted avg:
  * precision: 0.0000
  * recall: 0.0000
  * f1-score: 0.0000
  * support: 12315.0000
 accuracy:
  * 0.4214
________________________________________


Saving model to /fp/projects01/ec30/eirikeg/models
Model saved!

----------------------------------------------------------------------------------------------------



====================================================================================================
BEST MODEL (f1: 0.001577):
Training model: bert-base-multilingual-cased
 * 10 epochs
 * learning rate is 2e-05
 * train language: en-it
 * test language: en,it,de,sw,af,ru
 * dropout: 0.3
 * batch size is 32
 * frozen weights: True
 * activation: GELU
 * device is on cuda
 * warmup steps: 50

Saving model to /fp/projects01/ec30/eirikeg/models
Model saved!

----------------------------------------------------------------------------------------------------


Task and CPU usage stats:
JobID           JobName  AllocCPUS   NTasks     MinCPU MinCPUTask     AveCPU    Elapsed ExitCode 
------------ ---------- ---------- -------- ---------- ---------- ---------- ---------- -------- 
460142           in5550          4                                             00:17:28      0:0 
460142.batch      batch          4        1   00:17:18          0   00:17:18   00:17:28      0:0 
460142.exte+     extern          4        1   00:00:00          0   00:00:00   00:17:28      0:0 

Memory usage stats:
JobID            MaxRSS MaxRSSTask     AveRSS MaxPages   MaxPagesTask   AvePages 
------------ ---------- ---------- ---------- -------- -------------- ---------- 
460142                                                                           
460142.batch   2363084K          0   2363084K        0              0          0 
460142.exte+          0          0          0        0              0          0 

Disk usage stats:
JobID         MaxDiskRead MaxDiskReadTask    AveDiskRead MaxDiskWrite MaxDiskWriteTask   AveDiskWrite 
------------ ------------ --------------- -------------- ------------ ---------------- -------------- 
460142                                                                                                
460142.batch     4276.38M               0       4276.38M     3382.07M                0       3382.07M 
460142.exte+        0.01M               0          0.01M        0.00M                0          0.00M 

GPU usage stats:

Job 460142 completed at Wed Mar 13 11:13:56 CET 2024

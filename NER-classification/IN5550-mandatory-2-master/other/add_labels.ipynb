{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef16415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gzip\n",
    "from train_finetune_script import create_tokenized_labels, recombine_to_original_labels\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
    "import seqeval\n",
    "import seqeval.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from smart_open import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63fdb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/fp/homes01/u01/ec-eirikeg/mandatory_2/surprise/surprise_test_set.tsv\"\n",
    "output_file = f\"surprise_data.tsv\"\n",
    "base_model= \"bert-base-multilingual-cased\"\n",
    "fine_tuned_model_path = \"/fp/projects01/ec30/eirikeg/models/bert-base-multilingual-cased_(en-it-de-ru-th-en)_f1_0.8358.pth\"\n",
    "MAX_TOKEN_LENGTH = 500 # BERT's maximum token length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d08d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_num = {\"[PAD]\": 0,\n",
    "                \"[CLS]\": 1,\n",
    "                \"[SEP]\": 2,\n",
    "                \"B-ORG\": 3,\n",
    "                \"I-ORG\": 4,\n",
    "                \"B-LOC\": 5,\n",
    "                \"I-LOC\": 6,\n",
    "                \"B-PER\": 7,\n",
    "                \"I-PER\": 8,\n",
    "                \"O\": 9,\n",
    "            }\n",
    "\n",
    "num_to_label = {v: k for k, v in label_to_num.items()}\n",
    "\n",
    "sentences = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a9917",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2505f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize the model configuration using the same number of labels that you trained with.\n",
    "config = AutoConfig.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=10,\n",
    "    cache_dir=\"./cache\"\n",
    ")\n",
    "\n",
    "# Create a new model with the configuration.\n",
    "model = AutoModelForTokenClassification.from_config(config)\n",
    "\n",
    "# Load your previously saved state dictionary into this model.\n",
    "# Ensure `args.model` is the path to your `.pth` file that you saved with `torch.save`.\n",
    "model.load_state_dict(torch.load(fine_tuned_model_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, cache_dir=\"./cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c26194-c2dc-404f-8dff-0e0d9258103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_predict(cur_sent, cur_ranges):\n",
    "    # Tokenize the sentence\n",
    "                    sent_string = \" \".join(cur_sent)\n",
    "                    tokens = tokenizer(sent_string, return_offsets_mapping=True, truncation=False)\n",
    "                    token_ranges = tokens[\"offset_mapping\"]\n",
    "                    tokens[\"input_ids\"] = tokens[\"input_ids\"][1:-1] # Remove [CLS] and [SEP] tokens\n",
    "                    \n",
    "                    num_tokens_left = len(tokens[\"input_ids\"])\n",
    "                    \n",
    "                    token_labels = []\n",
    "                    token_index = 0 # Start index for slicing the tokens\n",
    "                    \n",
    "                    # Handle cases where the sentence is too long\n",
    "                    while num_tokens_left > 0:\n",
    "                        \n",
    "                        # Ensure we don't try to index out of bounds.\n",
    "                        # The -2 is to account for the [CLS] and [SEP] tokens\n",
    "                        chunk_size = min(num_tokens_left, MAX_TOKEN_LENGTH - 2)\n",
    "                        chunk_tokens = {key: tokens[key][token_index:token_index + chunk_size] for key in tokens}\n",
    "    \n",
    "                        # Manually add [CLS] and [SEP] token IDs to the input IDs for this chunk\n",
    "                        chunk_tokens[\"input_ids\"] = [101] + chunk_tokens[\"input_ids\"] + [102]\n",
    "                        \n",
    "                        # Manually add attention mask bits for [CLS] and [SEP] (these should be 1s)\n",
    "                        chunk_tokens[\"attention_mask\"] = [1] + chunk_tokens[\"attention_mask\"] + [1]\n",
    "    \n",
    "                        chunk_ranges = chunk_tokens[\"offset_mapping\"]\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            \n",
    "                            # Convert the tokens to tensors\n",
    "                            chunk_input_ids = torch.tensor(chunk_tokens[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "                            chunk_attention_mask = torch.tensor(chunk_tokens[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "                            \n",
    "                            if chunk_input_ids.shape[1] > MAX_TOKEN_LENGTH:\n",
    "                                print(f\"chunk_size: {chunk_size}, num_tokens_left: {num_tokens_left}, len(input_ids): {len(chunk_input_ids.squeeze())}\")\n",
    "                            \n",
    "                            # Get the model's prediction\n",
    "                            outputs = model(chunk_input_ids, attention_mask=chunk_attention_mask)\n",
    "                            predictions = torch.argmax(outputs.logits, dim=2).squeeze(0).cpu().numpy()\n",
    "                            predictions = [num_to_label[pred] for pred in predictions]\n",
    "                            \n",
    "                            # Remove the [CLS] and [SEP] tokens\n",
    "                            predictions = predictions[1:-1]\n",
    "                                \n",
    "                            token_labels.extend(predictions)\n",
    "                            \n",
    "                        # Prepare index for next slice\n",
    "                        token_index += chunk_size\n",
    "                        num_tokens_left -= chunk_size\n",
    "                    \n",
    "\n",
    "\n",
    "                    \n",
    "                    # Recombine labels to original word ranges\n",
    "                    pred_labels = recombine_to_original_labels(token_labels, cur_ranges, token_ranges)\n",
    "                    \n",
    "                    sentences.append(cur_sent)\n",
    "                    labels.append(pred_labels)\n",
    "                \n",
    "                    if len(pred_labels) != len(cur_sent):\n",
    "                        print(f\"tok_labels: {predictions}\")\n",
    "                        print(f\"cur_sent{cur_sent}\")\n",
    "                        print(f\"pred_labels{pred_labels}\\n\\n\")\n",
    "                        print(f\"Length mismatch {len(pred_labels)}, {len(cur_sent)}, {len(token_labels)}, {len(cur_ranges)}\")\n",
    "                        print(cur_sent)\n",
    "                        print(pred_labels)\n",
    "                        print(cur_ranges)\n",
    "                        print(chunk_ranges)\n",
    "                        print(tokens)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41fb9d30-0b8f-49ba-979b-3950f417e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tag_from_file(path):\n",
    "    with open(path, 'r') as file:\n",
    "                cur_sent = []\n",
    "                cur_ranges = []\n",
    "                prev_idx = 0\n",
    "                \n",
    "                for i, line in enumerate(file):\n",
    "                    line_string = line.split()\n",
    "                    # If line is a word, add to current sentence\n",
    "                    if line_string:\n",
    "                        word = line_string[0]\n",
    "                        \n",
    "                        \n",
    "                        # Build the word ranges for the sentence\n",
    "                        word_len = len(word)\n",
    "                        cur_ranges.append((prev_idx, prev_idx + word_len))\n",
    "                        prev_idx += word_len + 1 # add 1 for the space from later concatenation\n",
    "                        \n",
    "                        # Add the word and label to the current sentence\n",
    "                        cur_sent.append(word)\n",
    "    \n",
    "                    # At every new line: tokenize the sentence and get the model's predicted labels\n",
    "                    else:\n",
    "                        tokenize_and_predict(cur_sent, cur_ranges)\n",
    "                        cur_sent = []\n",
    "                        cur_labels = []\n",
    "                        cur_ranges = []\n",
    "                        prev_idx = 0  \n",
    "                        continue\n",
    "\n",
    "                # If file does not end on empty line\n",
    "                if len(cur_sent):\n",
    "                    tokenize_and_predict(cur_sent, cur_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2d5aa",
   "metadata": {},
   "source": [
    "# Convert sentence data to word-level output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b6c173-ab31-448d-8ded-8977f025a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_words(df):\n",
    "    words = []\n",
    "    labels = []\n",
    "    for i, row in df.iterrows():\n",
    "        sentence = row[\"sentence\"]\n",
    "        word_labels = row[\"labels\"]\n",
    "        words.extend(sentence)\n",
    "        labels.extend(word_labels)\n",
    "\n",
    "        # Add NaN row after each sentence\n",
    "        words.append(np.nan)\n",
    "        labels.append(np.nan)\n",
    "\n",
    "    return pd.DataFrame({'word': words, 'label': labels})\n",
    "\n",
    "def save_file(df_sent_to_word, name):\n",
    "    # Use \"\\n\" to directly represent newlines for empty rows\n",
    "    df_sent_to_word.to_csv(name, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61692144-4626-4840-8d7b-c7482c66c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tag_from_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae6666e9-0fab-4d79-91be-4bf555770899",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.DataFrame({\"sentence\": sentences, \"labels\": labels})\n",
    "\n",
    "word_df = convert_to_words(sent_df)\n",
    "save_file(word_df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a84c2f-1999-4042-8b2a-eeea82a2a805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47548"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8ac64-c158-4c03-8b2d-8238998f677b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

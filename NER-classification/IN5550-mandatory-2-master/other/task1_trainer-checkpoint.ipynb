{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca29153-2c52-4f2a-931d-229fbaf67cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikiann\", \"en\",cache_dir=\"./cache_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1ea6dbc-24c0-4683-ad49-0f7304e29468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# set TOKENIZERS_PARALLELISM so that it doesn't annoy us\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Train a model on the SNLI dataset')\n",
    "    parser.add_argument('--model', type=str, default='bert-base-multilingual-cased', help='The model to use')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='The batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=3, help='The number of epochs to train')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='The learning rate')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='The random seed')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=50, help='The number of warmup steps')\n",
    "    parser.add_argument('--gradient_clipping', type=float, default=10.0, help='The gradient clipping value')\n",
    "    return parser.parse_args([])\n",
    "\n",
    "args = parse_arguments()\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model,\n",
    "    cache_dir=\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3775d9ef-af17-4563-b72b-55145025a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /fp/homes01/u01/ec-\n",
      "[nltk_data]     eirikeg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78, 63, 70]\n",
      "('/fp/homes01/u01/ec-eirikeg/mandatory_2/data/train-en.tsv.gz', '/fp/homes01/u01/ec-eirikeg/mandatory_2/data/train-it.tsv.gz', '/fp/homes01/u01/ec-eirikeg/mandatory_2/data/train-gu.tsv.gz')\n"
     ]
    }
   ],
   "source": [
    "from smart_open import open\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import nltk \n",
    "\n",
    "nltk.download('punkt') \n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def find_substring_index(substring_list, string_list):\n",
    "    indexes = []\n",
    "    for substring in substring_list:\n",
    "        try:\n",
    "            index = next(i for i, string in enumerate(string_list) if substring in string)\n",
    "            indexes.append(index)\n",
    "        except StopIteration:\n",
    "            indexes.append(-1)\n",
    "    return indexes\n",
    "\n",
    "def import_func(sub_string_list,current_dir):\n",
    "    dir_path = current_dir\n",
    "    \n",
    "    # Construct the base path for the data directory\n",
    "    dir_path = os.path.join(dir_path, \"data\")\n",
    "    \n",
    "    files = os.listdir(dir_path)\n",
    "\n",
    "    list_paths = [os.path.join(dir_path, files[i]) for i in range(len(files))]\n",
    "    match_on = find_substring_index(sub_string_list, list_paths)\n",
    "    print(match_on)\n",
    "\n",
    "    path_get = itemgetter(*match_on)\n",
    "    get_paths = path_get(list_paths)\n",
    "    return get_paths\n",
    "\n",
    "def read_file_to_df(path_in_list):\n",
    "    print(path_in_list)\n",
    "    df_list = []\n",
    "    for path_in in path_in_list: \n",
    "        with open(path_in, \"rb\") as f:\n",
    "            df = pd.read_csv(f, sep=\"\\t\", header=0)\n",
    "            df_list.append(df)\n",
    "    return df_list\n",
    "current_directory = os.getcwd()\n",
    "paths_to_print=import_func([\"train-en.tsv.gz\",\"train-it.tsv.gz\",\"train-gu.tsv.gz\"],current_directory)\n",
    "first_doc=read_file_to_df(paths_to_print)[0]\n",
    "\n",
    "train_text_col = first_doc.iloc[:, 0].to_string(index=False)\n",
    "train_text=' '.join(train_text_col.split())\n",
    "\n",
    "sent_tokenizing=sent_tokenize(train_text)\n",
    "\n",
    "train_lab_col = first_doc.iloc[:, 1].to_string(index=False)\n",
    "train_labels=' '.join(train_lab_col.split())\n",
    "\n",
    "train_text_list=train_text_col.split()\n",
    "train_label_list=train_lab_col.split()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d7055e8-31f9-4110-ae6e-a60d6a3c1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'St.', 'Lawrence', 'River', ')', '(', '968', 'MW', ')', ';', \"'\", \"''\", 'Anders', 'Lindström', \"''\", \"'\", 'Karl', 'Ove', 'Knausgård', '(', 'born', '1968', ')', 'Atlantic', 'City', ',', 'New', 'Jersey', 'Her']\n",
      "['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_text_list[1:30])\n",
    "print(train_label_list[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87096f22-ab97-45a5-b602-40211653f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_and_labels(text_list, labels_list):\n",
    "    \"\"\"\n",
    "    Tokenize the input text into sentences and preserve the one-to-one correspondence\n",
    "    between tokens and labels.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text to tokenize.\n",
    "    - labels (str): The corresponding labels for each word in the text.\n",
    "\n",
    "    Returns:\n",
    "    - tokenized_texts (list): The tokenized sentences.\n",
    "    - tokenized_labels (list): The corresponding labels for each token.\n",
    "    \"\"\"\n",
    "\n",
    "    #joined_text=\" \".join(text)\n",
    "    # Tokenize sentences\n",
    "    #sentences = nltk.sent_tokenize(joined_text)\n",
    "\n",
    "    # Initialize lists to store tokenized text and labels\n",
    "    tokenize_nested_text=[]\n",
    "    tokenize_nested_label=[]\n",
    "    tokenized_texts = []\n",
    "    tokenized_labels = [] \n",
    "    # Keep track of the current index in the tokenized text\n",
    "    current_index = 0\n",
    "    len_text_list=len(text_list)\n",
    "    count_for=0\n",
    "    # Iterate through sentences\n",
    "    for i,word in enumerate(text_list):\n",
    "        count_for+=1\n",
    "        \n",
    "            \n",
    "        \n",
    "        if current_index<=10:\n",
    "            \n",
    "            tokenized_texts.append(word)\n",
    "            \n",
    "            if (\",\" in word or \".\" in word) and len(word)==1:\n",
    "               \n",
    "            # Append the corresponding label to the tokenized labels\n",
    "               tokenized_labels.append(\"O\")\n",
    "               current_index=current_index \n",
    "            else:\n",
    "                tokenized_labels.append(labels_list[i])\n",
    "                current_index+=1\n",
    "            \n",
    "        if current_index>10:\n",
    "            current_index=0\n",
    "            tokenized_texts = []\n",
    "            tokenized_labels = [] \n",
    "        \n",
    "        if current_index==10:\n",
    "            tokenize_nested_text.append(tokenized_texts)\n",
    "            tokenize_nested_label.append(tokenized_labels)\n",
    "        \n",
    "        if count_for==len_text_list:\n",
    "           break\n",
    "       \n",
    "        \n",
    "    return tokenize_nested_text, tokenize_nested_label\n",
    "\n",
    "\n",
    "train_text_list=train_text_col.split()\n",
    "train_label_list=train_lab_col.split()\n",
    "\n",
    "text_sent,label_sent=tokenize_text_and_labels(train_text_list, train_label_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e3aa56e-e0f2-40b9-9bcb-5b6174083a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import itertools\n",
    "all_labels = list(itertools.chain.from_iterable(label_sent))\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "# Apply LabelEncoder to each sublist\n",
    "numerical_NER = [list(label_encoder.transform(sublist_list)) for sublist_list in label_sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7642bf9-d3b6-47ee-8ac5-a6214ce92027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O']\n",
      "[0, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "numeric_flatt_unique=list(np.unique(np.array(list(chain.from_iterable(numerical_NER)))))\n",
    "all_labels_flatt_unique=list(np.unique(np.array(all_labels)))\n",
    "print(all_labels_flatt_unique)\n",
    "print(numeric_flatt_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bad3baad-f6a8-4522-a0c3-2ab35b91fb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.unique(np.array(all_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb012584-c657-4f8e-9662-dd428c23eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_NE_to_GNE(all_labels_flatt_unique,unique_NER):\n",
    "    unique_NER=[string[2:] for string in all_labels_flatt_unique if len(string)>2]\n",
    "    \n",
    "    unique_NER=set(unique_NER)\n",
    "    unique_NER=list(unique_NER)\n",
    " \n",
    "    index_NER_genral=[[numeric_flatt_unique[j]   \n",
    "       for j, all_labels_per in enumerate(all_labels_flatt_unique) if unique_ner in all_labels_per] \n",
    "       for i,unique_ner in  enumerate(unique_NER) \n",
    "      ]\n",
    "    index_NER_genral.append([numeric_flatt_unique[-1]])\n",
    "    \n",
    "    unique_NER=unique_NER+[\"O\"]\n",
    "    \n",
    "    dict_map_NER={unique_ner:index_ner_genral for unique_ner,index_ner_genral in zip(unique_NER,index_NER_genral)}\n",
    "    return dict_map_NER\n",
    "\n",
    "map=map_NE_to_GNE(all_labels_flatt_unique,numeric_flatt_unique)\n",
    "\n",
    "#create intervalls for target and prediction by using this to find indexes matching equal \n",
    "#label bought for target and predict vector to evaluate accuracy and F1.Reduce the len of evaluation vector\n",
    "#classify binary target = np.ones and prdict 010101 depending on accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "217cf6cd-15eb-4a00-8be8-9744f423630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map=list(map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6dc12f1-8bb6-41f2-94c8-27771d3bba71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d1892454ae46db89f5703903f2322a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "518\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "# Example nested lists of tokenized texts and their corresponding labels\n",
    "\n",
    "\n",
    "# Create a list of dictionaries where each dictionary represents an example\n",
    "\n",
    "\n",
    "data = {\"tokens\": text_sent, \"ner_tags\": numerical_NER,}\n",
    "    \n",
    "\n",
    "# Create a dataset from the list of dictionaries\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Create a dataset from the Pandas DataFrame\n",
    "data = Dataset.from_dict(data)\n",
    "\n",
    "# Instantiate tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\",dir_cache=\"./cache_2\")\n",
    "\n",
    "# Function to tokenize and adjust labels\n",
    "def tokenize_adjust_labels(all_samples_per_split):\n",
    "  tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"tokens\"], is_split_into_words=True)\n",
    "  #tokenized_samples is not a datasets object so this alone won't work with Trainer API, hence map is used \n",
    "  #so the new keys [input_ids, labels (after adjustment)]\n",
    "  #can be added to the datasets dict for each train test validation split\n",
    "  total_adjusted_labels = []\n",
    "  print(len(tokenized_samples[\"input_ids\"]))\n",
    "  for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
    "    prev_wid = -1\n",
    "    word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
    "    existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
    "    #print(existing_label_ids)\n",
    "      \n",
    "    i = -1\n",
    "    adjusted_label_ids = []\n",
    "   \n",
    "    for wid in word_ids_list:\n",
    "      if(wid is None):\n",
    "        adjusted_label_ids.append(-100)\n",
    "      elif(wid!=prev_wid):\n",
    "        i = i + 1\n",
    "        adjusted_label_ids.append(existing_label_ids[i])\n",
    "        prev_wid = wid\n",
    "      else:\n",
    "        label_name = all_labels[existing_label_ids[i]]\n",
    "        adjusted_label_ids.append(existing_label_ids[i])\n",
    "        \n",
    "    total_adjusted_labels.append(adjusted_label_ids)\n",
    "  tokenized_samples[\"labels\"] = total_adjusted_labels\n",
    "  return tokenized_samples\n",
    "\n",
    "\n",
    "# Tokenize and adjust labels for the entire dataset\n",
    "tokenized_data = data.map(tokenize_adjust_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "845c3dbc-63ef-4ce5-b929-0def0d1d504c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d562ffea0784d349e44262d3d512e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenized_data[2]['labels']\n",
    "#dataa=list(tokenized_data[2]['labels'])\n",
    "#indices_to_remove=[0,len(tokenized_data[2]['labels'])-1]\n",
    "from itertools import groupby\n",
    "def convert_id_to_NE(all_samples):\n",
    "    \n",
    "    input_labels,input_tokens=all_samples['labels'],all_samples[\"tokens\"]\n",
    "    \n",
    "    token_span=[]\n",
    "    NER_span=[]\n",
    "    for i in range(0, len(all_samples['labels'])):\n",
    "        indices_to_remove=[0,len(input_labels)-1]\n",
    "        new_list = [value for i, value in enumerate(input_labels[i]) if i not in indices_to_remove]\n",
    "        \n",
    "        result = [[key for key, values in map.items() if isinstance(values, list) and value in values ] \n",
    "                  for value in new_list]\n",
    "        \n",
    "        flatt_result=list(chain.from_iterable(result))\n",
    "        \n",
    "        grouped_tokens = [list(group) for key, group in groupby(zip(input_tokens[i], flatt_result), lambda x: x[1])]\n",
    "        \n",
    "        # Concatenate tokens for each group\n",
    "        token_sequences = [' '.join([token for token, _ in group]) for group in grouped_tokens]\n",
    "        new_sequence = [elem for i, elem in enumerate(flatt_result) if i == 0 or elem != flatt_result[i - 1]]\n",
    "        \n",
    "        token_span.append(token_sequences )\n",
    "        NER_span.append(new_sequence)\n",
    "    all_samples['NER_span']=token_span\n",
    "    all_samples['token_span']=NER_span\n",
    "    return all_samples\n",
    "\n",
    "#convert_id_to_NE(tokenized_data)\n",
    "\n",
    "tokenized_data_span = tokenized_data.map(convert_id_to_NE, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25674f-2603-4391-9de3-1df2840dc0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import accelerate\n",
    "from accelerate import Accelerator\n",
    "wandb.init(mode=\"disabled\")\n",
    "# Define the accelerator\n",
    "accelerator = Accelerator()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args.model,\n",
    "    cache_dir=\"./cache\",\n",
    "    trust_remote_code=True,\n",
    "    num_labels=len(all_labels_flatt_unique)\n",
    ")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:  # Assuming \"classifier\" is the name of your classification head\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Verify the trainable parameters\n",
    "#for name, param in model.named_parameters():\n",
    "    #print(name, param.requires_grad)\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tune_bert_output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=100,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 1000,\n",
    "    run_name = \"ep_10_tokenized_11\",\n",
    "    save_strategy='no'\n",
    "   \n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "  \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde4b9d-418f-458d-81ea-70156100d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row_ner_tags = tokenized_data[\"ner_tags\"][0]\n",
    "all_equal = all(tag == first_row_ner_tags for tag in tokenized_data[\"ner_tags\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ca6bb-6bd2-4c13-be88-2bf1d4b221a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(tokenizer.convert_ids_to_tokens(tokenized_data[30]['input_ids']))\n",
    "len(tokenized_data[30]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363d2bf-78a3-4e65-8ec7-694e34bf5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels_flatt_unique[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels_flatt_unique[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "      if(k not in flattened_results.keys()):\n",
    "        flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0204c-81db-4e19-9f2a-3ce9ae9e6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(all_labels)\n",
    "print(f\"{tokenized_data[3]['tokens']} \\n {tokenized_data[3]['ner_tags']}\")\n",
    "tokenizer.decode(tokenized_data[3]['input_ids'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

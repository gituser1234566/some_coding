{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing testing\n",
    "Download pre-trained preprocessor from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir=\"./cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The International Court of Justice has its seat in The Hague\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"R.H. Saunders ( St. Lawrence River ) ( 968 MW )\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre: 11, toks: 17\n",
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (5, 13), (14, 15), (16, 18), (18, 19), (20, 28), (29, 34), (35, 36), (37, 38), (39, 42), (43, 45), (46, 47), (0, 0)]\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(sentence, return_offsets_mapping=True)\n",
    "subword_ids = tokens[\"input_ids\"]\n",
    "offsets = tokens[\"offset_mapping\"]\n",
    "print(f\"pre: {len(sentence.split())}, toks: {len(subword_ids)}\")\n",
    "print(offsets)\n",
    "print(tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'R', '.', 'H', '.', 'Saunders', '(', 'St', '.', 'Lawrence', 'River', ')', '(', '968', 'MW', ')', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "subwords = tokenizer.convert_ids_to_tokens(subword_ids)\n",
    "\n",
    "print(subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1] * 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "---\n",
    "\n",
    "Sentences are split by empty lines, not `.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_label(label, num):\n",
    "    if num == 0:\n",
    "        return None\n",
    "    if label[0] == \"B\":\n",
    "        return [label] + [\"I\" + label[1:]] * (num - 1)\n",
    "    else:\n",
    "        return [label] * num\n",
    "\n",
    "def create_tokenized_labels(labels, original_ranges, token_ranges) -> List[int]:\n",
    "    new_labels = []\n",
    "    tok_id = 0\n",
    "    label_id = 0\n",
    "    cur_tok = token_ranges[tok_id]\n",
    "    tok_ranges = token_ranges[1:-1] # Remove start and end tokens\n",
    "   \n",
    "    # print(f\"ranges {tok_ranges}\")\n",
    "    # debug = False\n",
    "    # num_added = 0\n",
    "    # if len(token_ranges) >= 512:\n",
    "    #     print(f\"Printing debugs:\")\n",
    "    #     debug = False\n",
    "    \n",
    "    for start, end in original_ranges:\n",
    "        current_label = labels[label_id]\n",
    "        label_id += 1\n",
    "        counter = 0\n",
    "        while tok_id < len(tok_ranges) and cur_tok[1] <= end:  # Word spans multiple tokens\n",
    "            cur_tok = tok_ranges[tok_id]\n",
    "            counter += 1\n",
    "            tok_id += 1\n",
    "            if cur_tok[1] == end:\n",
    "                break\n",
    "\n",
    "        new_token_labels = split_label(current_label, counter)\n",
    "        if new_token_labels:\n",
    "            new_labels.extend(new_token_labels)\n",
    "\n",
    "        # if debug:\n",
    "        #     num_added += counter\n",
    "        #     print(f\" * * added {counter} to the now {num_added} long label-list ({len(new_labels)})\\n\")\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "def recombine_to_original_labels(tok_labels, original_ranges, token_ranges):\n",
    "    org_labels = []\n",
    "    tok_id = 0\n",
    "    label_id = 0\n",
    "\n",
    "    cur_tok = token_ranges[tok_id]\n",
    "    tok_ranges = token_ranges[1:-1]\n",
    "    \n",
    "    \n",
    "    for i, (start, end) in enumerate(original_ranges):\n",
    "        current_label = tok_labels[label_id]\n",
    "        # print(f\"Label: {current_label} Start: {start} End: {end}\")\n",
    "        \n",
    "        inner_labels = []\n",
    "        while cur_tok[1] <= end:\n",
    "            # print(f\" * cur_tok: {cur_tok } End: {end}\")\n",
    "            inner_labels.append(current_label)\n",
    "            \n",
    "            if tok_id >= len(tok_ranges) - 1:\n",
    "                break\n",
    "            \n",
    "            tok_id += 1 \n",
    "            label_id += 1\n",
    "            cur_tok = tok_ranges[tok_id]\n",
    "            \n",
    "        # print(f\" * inner: {inner_labels}\")\n",
    "        org_labels.append(inner_labels[0])\n",
    "    \n",
    "    return org_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'I-ORG', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O']\n",
      "['B-ORG', 'O', 'B-ORG', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = [\"B-ORG\", \"O\", \"B-ORG\", \"O\"]\n",
    "original_ranges = [       (0, 3),         (3, 5),         (6, 9),         (10, 14)]\n",
    "tok_ranges =      [(0,0), (0, 1), (2, 3), (3, 4), (4, 5), (6, 7), (8, 9), (10, 11), (12, 14), (0,0)]\n",
    "\n",
    "tok_labels = create_tokenized_labels(labels, original_ranges, tok_ranges)\n",
    "print(tok_labels)\n",
    "\n",
    "reconverted_labels = recombine_to_original_labels(tok_labels, original_ranges, tok_ranges)\n",
    "print(reconverted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XTREMEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, train=True, language='en'):\n",
    "        train_or_dev = 'train' if train else 'dev'\n",
    "        file_name = f\"{train_or_dev}-{language}.tsv.gz\"\n",
    "        path = os.path.join(path, file_name)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir=\"./cache\")\n",
    "        \n",
    "        \n",
    "        self.sentences = []\n",
    "        self.sent_labels = []\n",
    "        self.ranges = []\n",
    "        \n",
    "        self.tokens = []\n",
    "        self.token_labels = []\n",
    "        self.token_ranges = []\n",
    "\n",
    "        # Convert between numerical representation\n",
    "        self.label_to_num = {\"[PAD}\": 0, \"[CLS]\": 1, \"[SEP]\": 2}\n",
    "        self.num_to_label = {}\n",
    "        \n",
    "        self.import_data(path)\n",
    "        self.tokenize_data()\n",
    "        \n",
    "    def tokenize_data(self, max_len=512):\n",
    "        for i, (sent, labels, ranges) in enumerate(zip(self.sentences, self.sent_labels, self.ranges)):\n",
    "            sent_string = \" \".join(sent)\n",
    "            tokens = self.tokenizer(sent_string, return_offsets_mapping=True, max_length=512, truncation=True)\n",
    "            tok_ranges = tokens[\"offset_mapping\"]\n",
    "            # print(f\"len labs {len(labels)}, ranges: {len(ranges)}, toks: {len(tok_ranges)}\")\n",
    "            tok_labels = create_tokenized_labels(labels, ranges, tok_ranges)\n",
    "\n",
    "            if len(tok_labels) != len(tok_ranges) - 2:\n",
    "                error_sent = \" \".join(tokenizer.convert_ids_to_tokens(tokens['input_ids']))\n",
    "                print(f\"ERROR IN LEN IN INDEX {i} ({len(tok_labels)}) : ({len(tok_ranges) - 2})\")\n",
    "                # print(f\" * {error_sent}\\n\")\n",
    "\n",
    "            # if len(tok_ranges) > max_len or len(tok_labels) > max_len:\n",
    "            #     del self.sentences[i]\n",
    "            #     del self.sent_labels[i]\n",
    "            #     del self.ranges[i]\n",
    "            #     continue\n",
    "            \n",
    "            self.tokens.append(tokens)\n",
    "            self.token_ranges.append(tok_ranges)\n",
    "            self.token_labels.append(tok_labels)\n",
    "    \n",
    "            # original_check = recombine_to_original_labels(tok_labels, ranges, tok_ranges)\n",
    "            # print(f\"Recombined: {original_check}\")\n",
    "\n",
    "        # print(f\"Original: {self.sent_labels[0]}\")\n",
    "        # print(f\"num toks: {len(tok_ranges)}\\n\\nLabels:{self.token_labels}\\n\\nnum labels: {len(self.token_labels[0])}\")  \n",
    "\n",
    "        \n",
    "    def import_data(self, path):\n",
    "        counter = 0\n",
    "        \n",
    "        with gzip.open(path, 'r') as file:\n",
    "            cur_sent = []\n",
    "            cur_labels = []\n",
    "            cur_range = []\n",
    "            prev_idx = 0\n",
    "            \n",
    "            for line in file:\n",
    "                # New sentence if file contains an empty line\n",
    "                if not line.split():\n",
    "                    self.sentences.append(cur_sent)\n",
    "                    self.sent_labels.append(cur_labels)\n",
    "                    self.ranges.append(cur_range)\n",
    "                    cur_sent = []\n",
    "                    cur_labels = []\n",
    "                    cur_range = []\n",
    "                    prev_idx = 0\n",
    "                    \n",
    "                    # Temporary print and early stopping\n",
    "                    # counter += 1\n",
    "                    # if counter >= 5:\n",
    "                    #     # print(f\"sents: {[' '.join(sent) for sent in self.sentences]}\\n\\n\")\n",
    "                    #     # print(f\"labels: {[[self.num_to_label[n] for n in sent] for sent in self.sent_labels]}\")\n",
    "                    #     # print(f\"ranges: {self.ranges}\")\n",
    "                    #     break\n",
    "                    continue\n",
    "                    \n",
    "                    \n",
    "                word, label = line.decode().split()\n",
    "                \n",
    "                # Create unique numbered labels\n",
    "                if label not in self.label_to_num.keys():\n",
    "                    new_num = len(self.label_to_num)\n",
    "                    self.label_to_num[label] = new_num\n",
    "                    self.num_to_label[new_num] = label\n",
    "                    \n",
    "                # num = self.label_to_num[label]\n",
    "                num = label\n",
    "                \n",
    "                \n",
    "                # Build the word ranges for the sentence\n",
    "                word_len = len(word)\n",
    "                cur_range.append((prev_idx, prev_idx + word_len))\n",
    "                prev_idx += word_len + 1 # add 1 for the space from later concatenation\n",
    "                \n",
    "                # Add the word and label to the current sentence\n",
    "                cur_sent.append(word)\n",
    "                cur_labels.append(num)\n",
    "                \n",
    "    \n",
    "                \n",
    "    def tokens_to_input_format(tokens):\n",
    "        return NotImplemented\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {**self.tokens[idx]}\n",
    "        data[\"sentence\"] = self.sentences[idx]\n",
    "        data[\"token_labels\"] = self.token_labels[idx]\n",
    "        data[\"original_labels\"] = self.sent_labels[idx]\n",
    "        data[\"label_to_num\"] = self.label_to_num\n",
    "        data[\"num_to_label\"] = self.num_to_label\n",
    "\n",
    "        # print(f'\\n{\"=\"*30}')\n",
    "        # print(f\"original ranges ({len(self.ranges[idx])}): {self.ranges[idx]}\") \n",
    "        # for key in data:\n",
    "        #     print(f\" * {key} ({len(data[key])}): {data[key]}\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "\n",
    "            \n",
    "class CollateFunctor:\n",
    "    def __init__(self, tokenizer, max_len, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "        \n",
    "    def batch_to_device(self, batch):\n",
    "        new_batch = {}\n",
    "        for key, value in batch.items():\n",
    "            if torch.is_tensor(value):\n",
    "                new_batch[key] = value.to(self.device)\n",
    "            else:\n",
    "                new_batch[key] = value\n",
    "        return new_batch\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "        max_batch_len = max(len(sample[\"input_ids\"]) for sample in batch)\n",
    "        max_len = max(self.max_len, max_batch_len)\n",
    "        \n",
    "        # Iterate over each sample in the batch\n",
    "        for sample in batch:\n",
    "            # Pad or truncate input_ids, token_type_ids, attention_mask\n",
    "            toks_labels_numerical = [sample[\"label_to_num\"][label] for label in sample[\"token_labels\"]]\n",
    "            toks_labels_numerical = [1, *toks_labels_numerical, 2] # Add start and end token\n",
    "\n",
    "            cur_ids = sample['input_ids']\n",
    "            \n",
    "            input_ids.append(torch.tensor(cur_ids))\n",
    "            token_type_ids.append(torch.tensor(sample[\"token_type_ids\"]))\n",
    "            attention_mask.append(torch.tensor(sample[\"attention_mask\"]))\n",
    "            labels.append(torch.tensor(toks_labels_numerical + (len( input_ids)-len(cur_ids))*[0]))  \n",
    "            \n",
    "            # # Add padding to labels\n",
    "            # pad_size = (len(input_ids[i])-len(toks_labels_numerical))\n",
    "            # labels.append(torch.tensor(toks_labels_numerical + pad_size*[-100]))\n",
    "            \n",
    "        # Pad sequences to ensure uniform length\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0,)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "        labels = torch.stack([term.squeeze(0) for term in labels])\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "        inputs['labels'] = labels.clone().detach()\n",
    "\n",
    "        # print(inputs['labels'])\n",
    "         \n",
    "        return self.batch_to_device(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, device, val_loader=None):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(batch.keys())\n",
    "        # for key, value in batch.items():\n",
    "        #     print(f\"{key} : {value.shape}\")\n",
    "        \n",
    "        # forward pass \n",
    "        loss = model(**batch).loss\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for batch in tqdm(val_loader):\n",
    "        outputs = model(**batch)\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        # Mask the [CLS] and [SEP] token as well\n",
    "        for mask in attention_mask:\n",
    "            unmasked = torch.where(mask == 1)[0]\n",
    "            mask[unmasked[0]] = 0\n",
    "            mask[unmasked[-1]] = 0\n",
    "        \n",
    "        active_logits = outputs.logits.view(-1, outputs.logits.shape[-1])[attention_mask.view(-1) == 1]  \n",
    "        active_labels = batch['labels'].view(-1)[attention_mask.view(-1) == 1]\n",
    "\n",
    "        total_correct += (active_logits.argmax(dim=1) == active_labels).sum().item()  \n",
    "        total_samples += active_labels.shape[0]\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"total correct: {total_correct}, total samples: {total_samples}: ({accuracy})\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Train a model on the SNLI dataset')\n",
    "    parser.add_argument('--model', type=str, default='bert-base-multilingual-cased', help='The model to use')\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='The batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=3, help='The number of epochs to train')\n",
    "    parser.add_argument('--lr', type=float, default=2e-5, help='The learning rate')\n",
    "    parser.add_argument('--freeze', type=bool, default=True, help='If to freeze the earlier BERT weights')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='The random seed')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=50, help='The number of warmup steps')\n",
    "    parser.add_argument('--gradient_clipping', type=float, default=10.0, help='The gradient clipping value')\n",
    "    return parser.parse_args([])\n",
    "\n",
    "args = parse_arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================Training model: bert-base-multilingual-cased for 3 epochs\n",
      " * learning rate is 2e-05 * batch size is 8\n",
      " * BERT weights are frozen\n",
      " * device is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2499/2499 [00:25<00:00, 97.14it/s, loss=1.16, lr=1.34e-5]  \n",
      "100%|██████████| 2499/2499 [00:23<00:00, 106.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct: 89367, total samples: 218859: (0.4083313914438063)\n",
      "Epoch 1: validation accuracy = 40.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2499/2499 [00:25<00:00, 99.65it/s, loss=0.707, lr=6.71e-6] \n",
      "100%|██████████| 2499/2499 [00:23<00:00, 107.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct: 115092, total samples: 218907: (0.525757513464622)\n",
      "Epoch 2: validation accuracy = 52.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2499/2499 [00:24<00:00, 100.17it/s, loss=0.719, lr=0]      \n",
      "100%|██████████| 2499/2499 [00:23<00:00, 107.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct: 122153, total samples: 218888: (0.558061657103176)\n",
      "Epoch 3: validation accuracy = 55.81%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir=\"./cache\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", \n",
    "                                                        cache_dir=\"./cache\", \n",
    "                                                        num_labels=10).to(device)\n",
    "\n",
    "# Freeze all layers of the pre-trained BERT model  \n",
    "if args.freeze:\n",
    "    for name, param in model.bert.named_parameters():  \n",
    "        param.requires_grad = False \n",
    "    \n",
    "\n",
    "print(f\"{'='*40}\" +\n",
    "      f\"Training model: {args.model} for {args.epochs} epochs\\n * learning rate is {args.lr}\" +\n",
    "      f\" * batch size is {args.batch_size}\\n * BERT weights are frozen\\n * device is {device}\")\n",
    "\n",
    "collate = CollateFunctor(tokenizer, 512, device)\n",
    "\n",
    "dataset = XTREMEDataset(\"./data\")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                          batch_size=args.batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True,\n",
    "                                          collate_fn=collate)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=args.lr\n",
    ")\n",
    "lr_scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=len(data_loader) * args.epochs\n",
    ")\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train_epoch(model, data_loader, optimizer, lr_scheduler, device)\n",
    "    accuracy = evaluate(model, data_loader, device)\n",
    "    print(f\"Epoch {epoch + 1}: validation accuracy = {accuracy:.2%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2499/2499 [00:23<00:00, 106.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct: 122124, total samples: 218844: (0.5580413445193837)\n",
      "Test accuracy = 55.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = evaluate(model, data_loader, device)\n",
    "print(f\"Test accuracy = {test_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

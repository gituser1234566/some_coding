{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca29153-2c52-4f2a-931d-229fbaf67cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikiann\", \"en\",cache_dir=\"./cache_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ea6dbc-24c0-4683-ad49-0f7304e29468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# set TOKENIZERS_PARALLELISM so that it doesn't annoy us\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Train a model on the SNLI dataset')\n",
    "    parser.add_argument('--model', type=str, default='./bert-base-multilingual-cased', help='The model to use')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='The batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=3, help='The number of epochs to train')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='The learning rate')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='The random seed')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=50, help='The number of warmup steps')\n",
    "    parser.add_argument('--gradient_clipping', type=float, default=10.0, help='The gradient clipping value')\n",
    "    return parser.parse_args([])\n",
    "\n",
    "args = parse_arguments()\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model,\n",
    "    cache_dir=\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3775d9ef-af17-4563-b72b-55145025a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /fp/homes01/u01/ec-\n",
      "[nltk_data]     eirikeg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78, 63, 70]\n",
      "('/fp/homes01/u01/ec-eirikeg/mandatory_2/data/train-en.tsv.gz', '/fp/homes01/u01/ec-eirikeg/mandatory_2/data/train-it.tsv.gz', '/fp/homes01/u01/ec-eirikeg/mandatory_2/data/train-gu.tsv.gz')\n"
     ]
    }
   ],
   "source": [
    "from smart_open import open\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import nltk \n",
    "\n",
    "nltk.download('punkt') \n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def find_substring_index(substring_list, string_list):\n",
    "    indexes = []\n",
    "    for substring in substring_list:\n",
    "        try:\n",
    "            index = next(i for i, string in enumerate(string_list) if substring in string)\n",
    "            indexes.append(index)\n",
    "        except StopIteration:\n",
    "            indexes.append(-1)\n",
    "    return indexes\n",
    "\n",
    "def import_func(sub_string_list,current_dir):\n",
    "    dir_path = current_dir\n",
    "    \n",
    "    # Construct the base path for the data directory\n",
    "    dir_path = os.path.join(dir_path, \"data\")\n",
    "    \n",
    "    files = os.listdir(dir_path)\n",
    "\n",
    "    list_paths = [os.path.join(dir_path, files[i]) for i in range(len(files))]\n",
    "    match_on = find_substring_index(sub_string_list, list_paths)\n",
    "    print(match_on)\n",
    "\n",
    "    path_get = itemgetter(*match_on)\n",
    "    get_paths = path_get(list_paths)\n",
    "    return get_paths\n",
    "\n",
    "def read_file_to_df(path_in_list):\n",
    "    print(path_in_list)\n",
    "    df_list = []\n",
    "    for path_in in path_in_list: \n",
    "        with open(path_in, \"rb\") as f:\n",
    "            df = pd.read_csv(f, sep=\"\\t\", header=0)\n",
    "            df_list.append(df)\n",
    "    return df_list\n",
    "current_directory = os.getcwd()\n",
    "paths_to_print=import_func([\"train-en.tsv.gz\",\"train-it.tsv.gz\",\"train-gu.tsv.gz\"],current_directory)\n",
    "first_doc=read_file_to_df(paths_to_print)[0]\n",
    "\n",
    "train_text_col = first_doc.iloc[:, 0].to_string(index=False)\n",
    "train_text=' '.join(train_text_col.split())\n",
    "\n",
    "sent_tokenizing=sent_tokenize(train_text)\n",
    "\n",
    "train_lab_col = first_doc.iloc[:, 1].to_string(index=False)\n",
    "train_labels=' '.join(train_lab_col.split())\n",
    "\n",
    "train_text_list=train_text_col.split()\n",
    "train_label_list=train_lab_col.split()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7055e8-31f9-4110-ae6e-a60d6a3c1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'St.', 'Lawrence', 'River', ')', '(', '968', 'MW', ')', ';', \"'\", \"''\", 'Anders', 'Lindström', \"''\", \"'\", 'Karl', 'Ove', 'Knausgård', '(', 'born', '1968', ')', 'Atlantic', 'City', ',', 'New', 'Jersey', 'Her']\n",
      "['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_text_list[1:30])\n",
    "print(train_label_list[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87096f22-ab97-45a5-b602-40211653f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_and_labels(text_list, labels_list):\n",
    "    \"\"\"\n",
    "    Tokenize the input text into sentences and preserve the one-to-one correspondence\n",
    "    between tokens and labels.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text to tokenize.\n",
    "    - labels (str): The corresponding labels for each word in the text.\n",
    "\n",
    "    Returns:\n",
    "    - tokenized_texts (list): The tokenized sentences.\n",
    "    - tokenized_labels (list): The corresponding labels for each token.\n",
    "    \"\"\"\n",
    "\n",
    "    #joined_text=\" \".join(text)\n",
    "    # Tokenize sentences\n",
    "    #sentences = nltk.sent_tokenize(joined_text)\n",
    "\n",
    "    # Initialize lists to store tokenized text and labels\n",
    "    tokenize_nested_text=[]\n",
    "    tokenize_nested_label=[]\n",
    "    tokenized_texts = []\n",
    "    tokenized_labels = [] \n",
    "    # Keep track of the current index in the tokenized text\n",
    "    current_index = 0\n",
    "    len_text_list=len(text_list)\n",
    "    count_for=0\n",
    "    # Iterate through sentences\n",
    "    for i,word in enumerate(text_list):\n",
    "        count_for+=1\n",
    "        \n",
    "            \n",
    "        \n",
    "        if current_index<=10:\n",
    "            \n",
    "            tokenized_texts.append(word)\n",
    "            \n",
    "            if (\",\" in word or \".\" in word) and len(word)==1:\n",
    "               \n",
    "            # Append the corresponding label to the tokenized labels\n",
    "               tokenized_labels.append(\"O\")\n",
    "               current_index=current_index \n",
    "            else:\n",
    "                tokenized_labels.append(labels_list[i])\n",
    "                current_index+=1\n",
    "            \n",
    "        if current_index>10:\n",
    "            current_index=0\n",
    "            tokenized_texts = []\n",
    "            tokenized_labels = [] \n",
    "        \n",
    "        if current_index==10:\n",
    "            tokenize_nested_text.append(tokenized_texts)\n",
    "            tokenize_nested_label.append(tokenized_labels)\n",
    "        \n",
    "        if count_for==len_text_list:\n",
    "           break\n",
    "       \n",
    "        \n",
    "    return tokenize_nested_text, tokenize_nested_label\n",
    "\n",
    "\n",
    "train_text_list=train_text_col.split()\n",
    "train_label_list=train_lab_col.split()\n",
    "\n",
    "text_sent,label_sent=tokenize_text_and_labels(train_text_list, train_label_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3aa56e-e0f2-40b9-9bcb-5b6174083a16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      3\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(label_sent))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import itertools\n",
    "all_labels = list(itertools.chain.from_iterable(label_sent))\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "# Apply LabelEncoder to each sublist\n",
    "numerical_NER = [list(label_encoder.transform(sublist_list)) for sublist_list in label_sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7642bf9-d3b6-47ee-8ac5-a6214ce92027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "numeric_flatt_unique=list(np.unique(np.array(list(chain.from_iterable(numerical_NER)))))\n",
    "all_labels_flatt_unique=list(np.unique(np.array(all_labels)))\n",
    "print(all_labels_flatt_unique)\n",
    "print(numeric_flatt_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3baad-f6a8-4522-a0c3-2ab35b91fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.unique(np.array(all_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb012584-c657-4f8e-9662-dd428c23eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_NE_to_GNE(all_labels_flatt_unique,unique_NER):\n",
    "    unique_NER=[string[2:] for string in all_labels_flatt_unique if len(string)>2]\n",
    "    \n",
    "    unique_NER=set(unique_NER)\n",
    "    unique_NER=list(unique_NER)\n",
    " \n",
    "    index_NER_genral=[[numeric_flatt_unique[j]   \n",
    "       for j, all_labels_per in enumerate(all_labels_flatt_unique) if unique_ner in all_labels_per] \n",
    "       for i,unique_ner in  enumerate(unique_NER) \n",
    "      ]\n",
    "    index_NER_genral.append([numeric_flatt_unique[-1]])\n",
    "    \n",
    "    unique_NER=unique_NER+[\"O\"]\n",
    "    \n",
    "    dict_map_NER={unique_ner:index_ner_genral for unique_ner,index_ner_genral in zip(unique_NER,index_NER_genral)}\n",
    "    return dict_map_NER\n",
    "\n",
    "map=map_NE_to_GNE(all_labels_flatt_unique,numeric_flatt_unique)\n",
    "\n",
    "#create intervalls for target and prediction by using this to find indexes matching equal \n",
    "#label bought for target and predict vector to evaluate accuracy and F1.Reduce the len of evaluation vector\n",
    "#classify binary target = np.ones and prdict 010101 depending on accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dfd30-eabf-4ed8-84dc-6a1174ae9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc12f1-8bb6-41f2-94c8-27771d3bba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "# Example nested lists of tokenized texts and their corresponding labels\n",
    "\n",
    "\n",
    "# Create a list of dictionaries where each dictionary represents an example\n",
    "\n",
    "\n",
    "data = {\"tokens\": text_sent, \"ner_tags\": numerical_NER}\n",
    "    \n",
    "\n",
    "# Create a dataset from the list of dictionaries\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Create a dataset from the Pandas DataFrame\n",
    "data = Dataset.from_dict(data)\n",
    "\n",
    "# Instantiate tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\",dir_cache=\"./cache_2\")\n",
    "\n",
    "# Function to tokenize and adjust labels\n",
    "def tokenize_adjust_labels(all_samples_per_split):\n",
    "  tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"tokens\"],return_tensors='pt', padding=True, truncation=True ,is_split_into_words=True)\n",
    "  #tokenized_samples is not a datasets object so this alone won't work with Trainer API, hence map is used \n",
    "  #so the new keys [input_ids, labels (after adjustment)]\n",
    "  #can be added to the datasets dict for each train test validation split\n",
    "  total_adjusted_labels = []\n",
    "  print(len(tokenized_samples[\"input_ids\"]))\n",
    "  for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
    "    prev_wid = -1\n",
    "    word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
    "    existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
    "    #print(existing_label_ids)\n",
    "      \n",
    "    i = -1\n",
    "    adjusted_label_ids = []\n",
    "   \n",
    "    for wid in word_ids_list:\n",
    "      if(wid is None):\n",
    "        adjusted_label_ids.append(-100)\n",
    "      elif(wid!=prev_wid):\n",
    "        i = i + 1\n",
    "        adjusted_label_ids.append(existing_label_ids[i])\n",
    "        prev_wid = wid\n",
    "      else:\n",
    "        label_name = all_labels[existing_label_ids[i]]\n",
    "        adjusted_label_ids.append(existing_label_ids[i])\n",
    "        \n",
    "    total_adjusted_labels.append(adjusted_label_ids)\n",
    "  tokenized_samples[\"labels\"] = total_adjusted_labels\n",
    "  return tokenized_samples\n",
    "\n",
    "\n",
    "# Tokenize and adjust labels for the entire dataset\n",
    "tokenized_data = data.map(tokenize_adjust_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c9058-b14e-4f96-b0a9-1033ae70ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_span\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866d536-c558-4e27-b4c6-1b8a1f1a9524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def keys_from_value(dictionary, value_list):\n",
    "    keys = []\n",
    "    for value in value_list:\n",
    "        for key, val in dictionary.items():\n",
    "            \n",
    "            if val == value:\n",
    "                keys.append(key)\n",
    "    return keys\n",
    "\n",
    "\n",
    "def convert_id_to_NE(all_samples):\n",
    "    label_mapping=map\n",
    "    print(map)\n",
    "    input_labels, input_tokens = all_samples['labels'], all_samples[\"tokens\"]\n",
    "    #input_labels=np.array(input_labels) + 1\n",
    "    #input_labels=input_labels.tolist()\n",
    "    token_span = []\n",
    "    NER_span = []\n",
    "    merged_indices = []  # Store merged indices\n",
    "    \n",
    "    for i in range(len(all_samples['labels'])):\n",
    "        indices_to_remove = [0, len(input_labels[i]) - 1]\n",
    "        new_list = [value for i, value in enumerate(input_labels[i]) if value!=-100]\n",
    "        \n",
    "        # Merge consecutive labels using label mapping and track merged indices\n",
    "        merged_labels = []\n",
    "        merged_indices_per_sample = []\n",
    "        for idx, label in enumerate(new_list):\n",
    "            label_name = next((k for k, v in label_mapping.items() if label in v), None)\n",
    "            #print(label_name)\n",
    "            if idx==0:\n",
    "               #print(label)\n",
    "               merged_labels.append(label_mapping[label_name])  \n",
    "               merged_indices_per_sample.append([idx])\n",
    "            else:    \n",
    "               #print(idx)\n",
    "               if merged_labels[(len(merged_labels)-1)][-1] not in label_mapping[label_name]:\n",
    "                  merged_labels.append(label_mapping[label_name])\n",
    "                  merged_indices_per_sample.append([idx])\n",
    "               else:\n",
    "                  merged_indices_per_sample[-1].append(idx)\n",
    "        \n",
    "        # Merge consecutive tokens\n",
    "        #grouped_tokens = [list(group) for key, group in groupby(zip(input_tokens[i],merged_indices_per_sample ), lambda x: x[1])]\n",
    "        #token_sequences = [' '.join([token for token, _ in group]) for group in grouped_tokens]\n",
    "        \n",
    "        #token_span.append(token_sequences)\n",
    "        NER_span.append(merged_labels)\n",
    "        merged_indices.append(merged_indices_per_sample)\n",
    "\n",
    "   \n",
    "    #NER_span=list(itertools.chain.from_iterable(NER_span))\n",
    "    #merged_indices=list(itertools.chain.from_iterable(merged_indices))\n",
    "    #token_sequences = [[all_samples[\"tokens\"][idx] for idx in sublist] for sublist in merged_indices]\n",
    "    \n",
    "    NER_span=[keys_from_value(label_mapping, ner_span) for ner_span in NER_span]\n",
    "    \n",
    "    #print(NER_span)\n",
    "    #all_samples['NER_span'] = token_span\n",
    "    all_samples['token_span'] = NER_span\n",
    "    all_samples['merged_indices'] = merged_indices\n",
    "    \n",
    "    return all_samples\n",
    "\n",
    "\n",
    "\n",
    "tokenized_data_span = tokenized_data.map(convert_id_to_NE, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b9153-cf8b-4192-9431-4cbf2a21863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_data_span[\"token_span\"][100])\n",
    "print(tokenized_data_span[\"ner_tags\"][100])\n",
    "print(tokenized_data_span['merged_indices'][100])\n",
    "print(tokenized_data_span['labels'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01758c31-be76-4dfd-b6a6-e5a67e7a8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_span\n",
    "#len(tokenized_data_span[\"input_ids\"][200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eac6e6-f3b8-4e8e-a887-86a32fbf2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class util_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, label_vocab=None):\n",
    "\n",
    "\n",
    "        self.data=data\n",
    "        #self.tokens=data['tokens']\n",
    "\n",
    "        #self.labels=data['labels']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class CollateFunctor:\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "        \n",
    "        # Iterate over each sample in the batch\n",
    "        for sample in batch:\n",
    "            # Pad or truncate input_ids, token_type_ids, attention_mask\n",
    "            input_ids.append(torch.tensor(sample[\"input_ids\"]))\n",
    "            token_type_ids.append(torch.tensor(sample[\"token_type_ids\"]))\n",
    "            attention_mask.append(torch.tensor(sample[\"attention_mask\"]))\n",
    "            # Add padding to labels\n",
    "           \n",
    "            labels.append(torch.tensor(sample[\"labels\"] +  (len( input_ids)-len(sample[\"labels\"]))*[-100] ))\n",
    "        \n",
    "        # Pad sequences to ensure uniform length\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0,)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        labels=torch.stack([term.squeeze(0) for term in labels])\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "    \n",
    "        #print(self.tokenizer.pad_token_id)\n",
    "        #inputs['labels'] = torch.tensor(labels)\n",
    "         \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e20d53-dee6-4552-8933-73f71527e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = util_Dataset(tokenized_data)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "train_set, batch_size=args.batch_size, shuffle=True, drop_last=True,\n",
    "collate_fn=CollateFunctor(tokenizer, 40)\n",
    ")\n",
    "\n",
    "# Peek at the first batch\n",
    "for batch in train_loader:\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}:\\t{value.shape}\\t{value.dtype}\")\n",
    "    break\t\n",
    "\n",
    "\n",
    "print(\"Input subwords: \", end=\"\")\n",
    "for subword_id in batch['input_ids'][0]:\n",
    "    print('`' + tokenizer.decode(subword_id.item()) + '`', end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd12f1-1569-4fef-a229-cc7c7141567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# set TOKENIZERS_PARALLELISM so that it doesn't annoy us\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Train a model on the SNLI dataset')\n",
    "    parser.add_argument('--model', type=str, default='/fp/homes01/u01/ec-rasyed/2024/labs/06/mand2/bert-base-multilingual-cased', help='The model to use')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='The batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=1, help='The number of epochs to train')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='The learning rate')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='The random seed')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=50, help='The number of warmup steps')\n",
    "    parser.add_argument('--gradient_clipping', type=float, default=10.0, help='The gradient clipping value')\n",
    "    return parser.parse_args([])\n",
    "\n",
    "args = parse_arguments()\n",
    "\n",
    "# set random seed\n",
    "#torch.manual_seed(args.seed)\n",
    "\n",
    "# set device\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "\n",
    "# load tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #args.model,\n",
    "    #cache_dir=\"./cache\"\n",
    "#).to(device)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    args.model,\n",
    "    cache_dir=\"./cache\",\n",
    "    trust_remote_code=True,\n",
    "    num_labels=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577163cd-903d-4488-8d4a-50de7adbadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065afb32-11ac-4b4d-8ed0-992803b48b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        #batch = batch\n",
    "        #.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(batch[\"labels\"])\n",
    "        # forward pass\n",
    "        model_out = model(**batch)\n",
    "        #print(model_out.logits)\n",
    "        \n",
    "        \n",
    "        loss = model_out.loss\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        lr_scheduler.step()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for batch in tqdm(val_loader):\n",
    "        batch = batch\n",
    "        #.to(device)\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        #total_correct += (outputs.logits.argmax(dim=1) == batch['labels']).sum().item()\n",
    "        #total_samples += batch['labels'].shape[0]\n",
    "\n",
    "    #accuracy = total_correct / total_samples\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from seqeval.metrics import classification_report\n",
    "#metric = load_metric(\"seqeval\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(model,data):\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for batch in tqdm(data):\n",
    "        batch = batch\n",
    "        #.to(device)\n",
    "        outputs = model(**batch)\n",
    "        predictions=outputs.logits\n",
    "        labels=batch[\"labels\"]\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        filtered_predictions = []\n",
    "        filtered_labels = []\n",
    "        for pred_seq, label_seq in zip(predictions, labels):\n",
    "            pred_seq_filtered = []\n",
    "            label_seq_filtered = []\n",
    "            for pred, label in zip(pred_seq, label_seq):\n",
    "                if label != -100:\n",
    "                    pred_seq_filtered.append(pred)\n",
    "                    label_seq_filtered.append(label)\n",
    "            filtered_predictions.append(pred_seq_filtered)\n",
    "            filtered_labels.append(label_seq_filtered)\n",
    "\n",
    "           \n",
    "        for pred_seq, label_seq in zip(filtered_predictions, filtered_labels):\n",
    "            pred_labels = [all_labels_flatt_unique[pred] for pred in pred_seq]\n",
    "            true_labels = [all_labels_flatt_unique[label] for label in label_seq]\n",
    "            all_predictions.append(pred_labels)\n",
    "            all_labels.append(true_labels)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    #print(all_labels[1:100])\n",
    "   # print(all_predictions[1:100])\n",
    "    return report\n",
    "\n",
    "\n",
    "\n",
    "all_labels_flatt_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0fcd5e-77c2-4150-b5c5-7d97a1e9f7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31155b4-8d51-414f-a4c1-8e828b7ed0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=args.lr\n",
    ")\n",
    "lr_scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=len(train_loader) * args.epochs\n",
    ")\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train_epoch(model, train_loader, optimizer, lr_scheduler, device)\n",
    "    eval_metric=compute_metrics(model,train_loader)\n",
    "    print(eval_metric)\n",
    "    #accuracy = evaluate(model, train_loader, device)\n",
    "    #print(f\"Epoch {epoch + 1}: validation accuracy = {accuracy:.2%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde4b9d-418f-458d-81ea-70156100d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row_ner_tags = tokenized_data[\"ner_tags\"][0]\n",
    "all_equal = all(tag == first_row_ner_tags for tag in tokenized_data[\"ner_tags\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ca6bb-6bd2-4c13-be88-2bf1d4b221a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(tokenizer.convert_ids_to_tokens(tokenized_data[30]['input_ids']))\n",
    "len(tokenized_data[30]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363d2bf-78a3-4e65-8ec7-694e34bf5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels_flatt_unique[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels_flatt_unique[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "      if(k not in flattened_results.keys()):\n",
    "        flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0204c-81db-4e19-9f2a-3ce9ae9e6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(all_labels)\n",
    "print(f\"{tokenized_data[3]['tokens']} \\n {tokenized_data[3]['ner_tags']}\")\n",
    "tokenizer.decode(tokenized_data[3]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b61fa-f11e-4f42-b79c-79996fb1b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# Example nested lists of predictions and true labels\n",
    "all_predictions = [['O', 'B-PER', 'I-PER', 'O'], ['B-LOC', 'I-LOC', 'O']]\n",
    "all_labels = [['O', 'B-PER', 'I-PER', 'O'], ['B-LOC', 'I-LOC', 'O']]\n",
    "\n",
    "# Compute classification report\n",
    "report = classification_report(all_labels, all_predictions)\n",
    "\n",
    "# Print the report\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

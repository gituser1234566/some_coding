{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ca29153-2c52-4f2a-931d-229fbaf67cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikiann\", \"en\",cache_dir=\"./cache_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ea6dbc-24c0-4683-ad49-0f7304e29468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# set TOKENIZERS_PARALLELISM so that it doesn't annoy us\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Train a model on the SNLI dataset')\n",
    "    parser.add_argument('--model', type=str, default='/fp/homes01/u01/ec-rasyed/2024/labs/06/mand2/bert-base-multilingual-cased', help='The model to use')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='The batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=3, help='The number of epochs to train')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='The learning rate')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='The random seed')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=50, help='The number of warmup steps')\n",
    "    parser.add_argument('--gradient_clipping', type=float, default=10.0, help='The gradient clipping value')\n",
    "    return parser.parse_args([])\n",
    "\n",
    "args = parse_arguments()\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model,\n",
    "    cache_dir=\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3775d9ef-af17-4563-b72b-55145025a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /fp/homes01/u01/ec-\n",
      "[nltk_data]     rasyed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78, 63, 70]\n",
      "('/fp/homes01/u01/ec-rasyed/2024/labs/06/mand2/data/train-en.tsv.gz', '/fp/homes01/u01/ec-rasyed/2024/labs/06/mand2/data/train-it.tsv.gz', '/fp/homes01/u01/ec-rasyed/2024/labs/06/mand2/data/train-gu.tsv.gz')\n"
     ]
    }
   ],
   "source": [
    "from smart_open import open\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import nltk \n",
    "\n",
    "nltk.download('punkt') \n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def find_substring_index(substring_list, string_list):\n",
    "    indexes = []\n",
    "    for substring in substring_list:\n",
    "        try:\n",
    "            index = next(i for i, string in enumerate(string_list) if substring in string)\n",
    "            indexes.append(index)\n",
    "        except StopIteration:\n",
    "            indexes.append(-1)\n",
    "    return indexes\n",
    "\n",
    "def import_func(sub_string_list,current_dir):\n",
    "    dir_path = current_dir\n",
    "    \n",
    "    # Construct the base path for the data directory\n",
    "    dir_path = os.path.join(dir_path, \"data\")\n",
    "    \n",
    "    files = os.listdir(dir_path)\n",
    "\n",
    "    list_paths = [os.path.join(dir_path, files[i]) for i in range(len(files))]\n",
    "    match_on = find_substring_index(sub_string_list, list_paths)\n",
    "    print(match_on)\n",
    "\n",
    "    path_get = itemgetter(*match_on)\n",
    "    get_paths = path_get(list_paths)\n",
    "    return get_paths\n",
    "\n",
    "def read_file_to_df(path_in_list):\n",
    "    print(path_in_list)\n",
    "    df_list = []\n",
    "    for path_in in path_in_list: \n",
    "        with open(path_in, \"rb\") as f:\n",
    "            df = pd.read_csv(f, sep=\"\\t\", header=0)\n",
    "            df_list.append(df)\n",
    "    return df_list\n",
    "current_directory = os.getcwd()\n",
    "paths_to_print=import_func([\"train-en.tsv.gz\",\"train-it.tsv.gz\",\"train-gu.tsv.gz\"],current_directory)\n",
    "first_doc=read_file_to_df(paths_to_print)[0]\n",
    "\n",
    "train_text_col = first_doc.iloc[:, 0].to_string(index=False)\n",
    "train_text=' '.join(train_text_col.split())\n",
    "\n",
    "sent_tokenizing=sent_tokenize(train_text)\n",
    "\n",
    "train_lab_col = first_doc.iloc[:, 1].to_string(index=False)\n",
    "train_labels=' '.join(train_lab_col.split())\n",
    "\n",
    "train_text_list=train_text_col.split()\n",
    "train_label_list=train_lab_col.split()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d7055e8-31f9-4110-ae6e-a60d6a3c1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'St.', 'Lawrence', 'River', ')', '(', '968', 'MW', ')', ';', \"'\", \"''\", 'Anders', 'Lindström', \"''\", \"'\", 'Karl', 'Ove', 'Knausgård', '(', 'born', '1968', ')', 'Atlantic', 'City', ',', 'New', 'Jersey', 'Her']\n",
      "['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_text_list[1:30])\n",
    "print(train_label_list[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87096f22-ab97-45a5-b602-40211653f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_and_labels(text_list, labels_list):\n",
    "    \"\"\"\n",
    "    Tokenize the input text into sentences and preserve the one-to-one correspondence\n",
    "    between tokens and labels.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text to tokenize.\n",
    "    - labels (str): The corresponding labels for each word in the text.\n",
    "\n",
    "    Returns:\n",
    "    - tokenized_texts (list): The tokenized sentences.\n",
    "    - tokenized_labels (list): The corresponding labels for each token.\n",
    "    \"\"\"\n",
    "\n",
    "    #joined_text=\" \".join(text)\n",
    "    # Tokenize sentences\n",
    "    #sentences = nltk.sent_tokenize(joined_text)\n",
    "\n",
    "    # Initialize lists to store tokenized text and labels\n",
    "    tokenize_nested_text=[]\n",
    "    tokenize_nested_label=[]\n",
    "    tokenized_texts = []\n",
    "    tokenized_labels = [] \n",
    "    # Keep track of the current index in the tokenized text\n",
    "    current_index = 0\n",
    "    len_text_list=len(text_list)\n",
    "    count_for=0\n",
    "    # Iterate through sentences\n",
    "    for i,word in enumerate(text_list):\n",
    "        count_for+=1\n",
    "        \n",
    "            \n",
    "        \n",
    "        if current_index<=10:\n",
    "            \n",
    "            tokenized_texts.append(word)\n",
    "            \n",
    "            if (\",\" in word or \".\" in word) and len(word)==1:\n",
    "               \n",
    "            # Append the corresponding label to the tokenized labels\n",
    "               tokenized_labels.append(\"O\")\n",
    "               current_index=current_index \n",
    "            else:\n",
    "                tokenized_labels.append(labels_list[i])\n",
    "                current_index+=1\n",
    "            \n",
    "        if current_index>10:\n",
    "            current_index=0\n",
    "            tokenized_texts = []\n",
    "            tokenized_labels = [] \n",
    "        \n",
    "        if current_index==10:\n",
    "            tokenize_nested_text.append(tokenized_texts)\n",
    "            tokenize_nested_label.append(tokenized_labels)\n",
    "        \n",
    "        if count_for==len_text_list:\n",
    "           break\n",
    "       \n",
    "        \n",
    "    return tokenize_nested_text, tokenize_nested_label\n",
    "\n",
    "\n",
    "train_text_list=train_text_col.split()\n",
    "train_label_list=train_lab_col.split()\n",
    "\n",
    "text_sent,label_sent=tokenize_text_and_labels(train_text_list, train_label_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3aa56e-e0f2-40b9-9bcb-5b6174083a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import itertools\n",
    "all_labels = list(itertools.chain.from_iterable(label_sent))\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "# Apply LabelEncoder to each sublist\n",
    "numerical_NER = [list(label_encoder.transform(sublist_list)) for sublist_list in label_sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7642bf9-d3b6-47ee-8ac5-a6214ce92027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O']\n",
      "[0, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "numeric_flatt_unique=list(np.unique(np.array(list(chain.from_iterable(numerical_NER)))))\n",
    "all_labels_flatt_unique=list(np.unique(np.array(all_labels)))\n",
    "print(all_labels_flatt_unique)\n",
    "print(numeric_flatt_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bad3baad-f6a8-4522-a0c3-2ab35b91fb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.unique(np.array(all_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb012584-c657-4f8e-9662-dd428c23eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_NE_to_GNE(all_labels_flatt_unique,unique_NER):\n",
    "    unique_NER=[string[2:] for string in all_labels_flatt_unique if len(string)>2]\n",
    "    \n",
    "    unique_NER=set(unique_NER)\n",
    "    unique_NER=list(unique_NER)\n",
    " \n",
    "    index_NER_genral=[[numeric_flatt_unique[j]   \n",
    "       for j, all_labels_per in enumerate(all_labels_flatt_unique) if unique_ner in all_labels_per] \n",
    "       for i,unique_ner in  enumerate(unique_NER) \n",
    "      ]\n",
    "    index_NER_genral.append([numeric_flatt_unique[-1]])\n",
    "    \n",
    "    unique_NER=unique_NER+[\"O\"]\n",
    "    \n",
    "    dict_map_NER={unique_ner:index_ner_genral for unique_ner,index_ner_genral in zip(unique_NER,index_NER_genral)}\n",
    "    return dict_map_NER\n",
    "\n",
    "map=map_NE_to_GNE(all_labels_flatt_unique,numeric_flatt_unique)\n",
    "\n",
    "#create intervalls for target and prediction by using this to find indexes matching equal \n",
    "#label bought for target and predict vector to evaluate accuracy and F1.Reduce the len of evaluation vector\n",
    "#classify binary target = np.ones and prdict 010101 depending on accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "217cf6cd-15eb-4a00-8be8-9744f423630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "map=list(map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6dc12f1-8bb6-41f2-94c8-27771d3bba71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73959ceba3341f88a8d17babceee52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "518\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "# Example nested lists of tokenized texts and their corresponding labels\n",
    "\n",
    "\n",
    "# Create a list of dictionaries where each dictionary represents an example\n",
    "\n",
    "\n",
    "data = {\"tokens\": text_sent, \"ner_tags\": numerical_NER,}\n",
    "    \n",
    "\n",
    "# Create a dataset from the list of dictionaries\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Create a dataset from the Pandas DataFrame\n",
    "data = Dataset.from_dict(data)\n",
    "\n",
    "# Instantiate tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\",dir_cache=\"./cache_2\")\n",
    "\n",
    "# Function to tokenize and adjust labels\n",
    "def tokenize_adjust_labels(all_samples_per_split):\n",
    "  tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"tokens\"],return_tensors='pt', padding=True, truncation=True ,is_split_into_words=True)\n",
    "  #tokenized_samples is not a datasets object so this alone won't work with Trainer API, hence map is used \n",
    "  #so the new keys [input_ids, labels (after adjustment)]\n",
    "  #can be added to the datasets dict for each train test validation split\n",
    "  total_adjusted_labels = []\n",
    "  print(len(tokenized_samples[\"input_ids\"]))\n",
    "  for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
    "    prev_wid = -1\n",
    "    word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
    "    existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
    "    #print(existing_label_ids)\n",
    "      \n",
    "    i = -1\n",
    "    adjusted_label_ids = []\n",
    "   \n",
    "    for wid in word_ids_list:\n",
    "      if(wid is None):\n",
    "        adjusted_label_ids.append(-100)\n",
    "      elif(wid!=prev_wid):\n",
    "        i = i + 1\n",
    "        adjusted_label_ids.append(existing_label_ids[i])\n",
    "        prev_wid = wid\n",
    "      else:\n",
    "        label_name = all_labels[existing_label_ids[i]]\n",
    "        adjusted_label_ids.append(existing_label_ids[i])\n",
    "        \n",
    "    total_adjusted_labels.append(adjusted_label_ids)\n",
    "  tokenized_samples[\"labels\"] = total_adjusted_labels\n",
    "  return tokenized_samples\n",
    "\n",
    "\n",
    "# Tokenize and adjust labels for the entire dataset\n",
    "tokenized_data = data.map(tokenize_adjust_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "845c3dbc-63ef-4ce5-b929-0def0d1d504c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac07f1eb7d074792bac893318acd834a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenized_data[2]['labels']\n",
    "#dataa=list(tokenized_data[2]['labels'])\n",
    "#indices_to_remove=[0,len(tokenized_data[2]['labels'])-1]\n",
    "from itertools import groupby\n",
    "def convert_id_to_NE(all_samples):\n",
    "    \n",
    "    input_labels,input_tokens=all_samples['labels'],all_samples[\"tokens\"]\n",
    "    \n",
    "    token_span=[]\n",
    "    NER_span=[]\n",
    "    for i in range(0, len(all_samples['labels'])):\n",
    "        indices_to_remove=[0,len(input_labels)-1]\n",
    "        new_list = [value for i, value in enumerate(input_labels[i]) if i not in indices_to_remove]\n",
    "        \n",
    "        result = [[key for key, values in map.items() if isinstance(values, list) and value in values ] \n",
    "                  for value in new_list]\n",
    "        \n",
    "        flatt_result=list(chain.from_iterable(result))\n",
    "        \n",
    "        grouped_tokens = [list(group) for key, group in groupby(zip(input_tokens[i], flatt_result), lambda x: x[1])]\n",
    "        \n",
    "        # Concatenate tokens for each group\n",
    "        token_sequences = [' '.join([token for token, _ in group]) for group in grouped_tokens]\n",
    "        new_sequence = [elem for i, elem in enumerate(flatt_result) if i == 0 or elem != flatt_result[i - 1]]\n",
    "        \n",
    "        token_span.append(token_sequences )\n",
    "        NER_span.append(new_sequence)\n",
    "    all_samples['NER_span']=token_span\n",
    "    all_samples['token_span']=NER_span\n",
    "    return all_samples\n",
    "\n",
    "#convert_id_to_NE(tokenized_data)\n",
    "\n",
    "tokenized_data_span = tokenized_data.map(convert_id_to_NE, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "338b9153-cf8b-4192-9431-4cbf2a21863d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'NER_span': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'token_span': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data_span.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01758c31-be76-4dfd-b6a6-e5a67e7a8b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['of',\n",
       "  'Arlington',\n",
       "  ',',\n",
       "  'which',\n",
       "  'comprises',\n",
       "  'Northern',\n",
       "  'Virginia',\n",
       "  '.',\n",
       "  '**',\n",
       "  'Bishopric',\n",
       "  'of',\n",
       "  'Hildesheim',\n",
       "  '-'],\n",
       " 'ner_tags': [3, 3, 6, 6, 6, 0, 3, 6, 6, 0, 3, 3, 6],\n",
       " 'input_ids': [101,\n",
       "  10108,\n",
       "  67799,\n",
       "  117,\n",
       "  10319,\n",
       "  58633,\n",
       "  15352,\n",
       "  13634,\n",
       "  119,\n",
       "  115,\n",
       "  115,\n",
       "  17576,\n",
       "  18570,\n",
       "  10108,\n",
       "  77003,\n",
       "  118,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100],\n",
       " 'NER_span': ['of Arlington',\n",
       "  ', which comprises',\n",
       "  'Northern Virginia',\n",
       "  '. ** Bishopric',\n",
       "  'of Hildesheim -'],\n",
       " 'token_span': ['LOC', 'O', 'LOC', 'O', 'LOC', 'O']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data_span[1000]\n",
    "#len(tokenized_data_span[\"input_ids\"][200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53eac6e6-f3b8-4e8e-a887-86a32fbf2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class util_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, label_vocab=None):\n",
    "\n",
    "\n",
    "        self.data=data\n",
    "        #self.tokens=data['tokens']\n",
    "\n",
    "        #self.labels=data['labels']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class CollateFunctor:\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "        \n",
    "        # Iterate over each sample in the batch\n",
    "        for sample in batch:\n",
    "            # Pad or truncate input_ids, token_type_ids, attention_mask\n",
    "            input_ids.append(torch.tensor(sample[\"input_ids\"]))\n",
    "            token_type_ids.append(torch.tensor(sample[\"token_type_ids\"]))\n",
    "            attention_mask.append(torch.tensor(sample[\"attention_mask\"]))\n",
    "            # Add padding to labels\n",
    "           \n",
    "            labels.append(torch.tensor(sample[\"labels\"] +  (len( input_ids)-len(sample[\"labels\"]))*[-100] ))\n",
    "        \n",
    "        # Pad sequences to ensure uniform length\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0,)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        labels=torch.stack([term.squeeze(0) for term in labels])\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "    \n",
    "\n",
    "        #inputs['labels'] = torch.tensor(labels)\n",
    "         \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6370200a-644a-4763-bc1d-8edb9ad9b4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14518"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1e20d53-dee6-4552-8933-73f71527e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\ttorch.Size([1, 39])\ttorch.int64\n",
      "token_type_ids:\ttorch.Size([1, 39])\ttorch.int64\n",
      "attention_mask:\ttorch.Size([1, 39])\ttorch.int64\n",
      "labels:\ttorch.Size([1, 39])\ttorch.int64\n"
     ]
    }
   ],
   "source": [
    "train_set = util_Dataset(tokenized_data_span)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "train_set, batch_size=args.batch_size, shuffle=True, drop_last=True,\n",
    "collate_fn=CollateFunctor(tokenizer, 40)\n",
    ")\n",
    "\n",
    "# Peek at the first batch\n",
    "for batch in train_loader:\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}:\\t{value.shape}\\t{value.dtype}\")\n",
    "    break\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9cd12f1-1569-4fef-a229-cc7c7141567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /fp/homes01/u01/ec-rasyed/2024/labs/06/mand2/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# set TOKENIZERS_PARALLELISM so that it doesn't annoy us\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Train a model on the SNLI dataset')\n",
    "    parser.add_argument('--model', type=str, default='/fp/homes01/u01/ec-rasyed/2024/labs/06/mand2/bert-base-multilingual-cased', help='The model to use')\n",
    "    parser.add_argument('--batch_size', type=int, default=1, help='The batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=2, help='The number of epochs to train')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='The learning rate')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='The random seed')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=50, help='The number of warmup steps')\n",
    "    parser.add_argument('--gradient_clipping', type=float, default=10.0, help='The gradient clipping value')\n",
    "    return parser.parse_args([])\n",
    "\n",
    "args = parse_arguments()\n",
    "\n",
    "# set random seed\n",
    "#torch.manual_seed(args.seed)\n",
    "\n",
    "# set device\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "\n",
    "# load tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #args.model,\n",
    "    #cache_dir=\"./cache\"\n",
    "#).to(device)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args.model,\n",
    "    cache_dir=\"./cache\",\n",
    "    trust_remote_code=True,\n",
    "    num_labels=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "577163cd-903d-4488-8d4a-50de7adbadf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "065afb32-11ac-4b4d-8ed0-992803b48b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        batch = batch\n",
    "        #.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        input_ids=batch[\"input_ids\"]\n",
    "        labels=batch[\"labels\"]\n",
    "        batch_new=batch\n",
    "        #print(labels.shape)\n",
    "        #print(input_ids.shape)\n",
    "        print(batch)\n",
    "        # forward pass\n",
    "        loss = model(**batch).loss\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        lr_scheduler.step()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for batch in tqdm(val_loader):\n",
    "        batch = batch\n",
    "        #.to(device)\n",
    "        outputs = model(**batch)\n",
    "        total_correct += (outputs.logits.argmax(dim=1) == batch['labels']).sum().item()\n",
    "        total_samples += batch['labels'].shape[0]\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a31155b4-8d51-414f-a4c1-8e828b7ed0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                       | 0/14518 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 10146,   169, 21776,   119, 10607,   100, 12172, 10472, 33400,\n",
      "         13898, 10108, 18003,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    6,    6,    1,    6,    1,    6,    6,    6,    6,    1,    4,\n",
      "            4, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (39).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mget_linear_schedule_with_warmup(\n\u001b[1;32m      5\u001b[0m     optimizer, num_warmup_steps\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_steps, num_training_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mepochs\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: validation accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 15\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, lr_scheduler, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/fp/projects01/ec30/software/easybuild/software/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fp/projects01/ec30/software/easybuild/software/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1599\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1598\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1599\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1601\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[0;32m/fp/projects01/ec30/software/easybuild/software/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fp/projects01/ec30/software/easybuild/software/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fp/projects01/ec30/software/easybuild/software/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fp/projects01/ec30/software/easybuild/software/nlpl-pytorch/2.1.2-foss-2022b-cuda-12.0.0-Python-3.10.8/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (39)."
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=args.lr\n",
    ")\n",
    "lr_scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=len(train_loader) * args.epochs\n",
    ")\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train_epoch(model, train_loader, optimizer, lr_scheduler, device)\n",
    "    accuracy = evaluate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch + 1}: validation accuracy = {accuracy:.2%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fdde4b9d-418f-458d-81ea-70156100d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row_ner_tags = tokenized_data[\"ner_tags\"][0]\n",
    "all_equal = all(tag == first_row_ner_tags for tag in tokenized_data[\"ner_tags\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "340ca6bb-6bd2-4c13-be88-2bf1d4b221a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(tokenizer.convert_ids_to_tokens(tokenized_data[30]['input_ids']))\n",
    "len(tokenized_data[30]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7363d2bf-78a3-4e65-8ec7-694e34bf5934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fp/homes01/u01/ec-rasyed/.local/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels_flatt_unique[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels_flatt_unique[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "      if(k not in flattened_results.keys()):\n",
    "        flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "edc0204c-81db-4e19-9f2a-3ce9ae9e6a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['second', 'marriage', 'was', 'Marie', \"d'Agoult\", '(', '1805–1876', ')', ',', 'who', 'in'] \n",
      " [6, 6, 6, 2, 5, 6, 6, 6, 6, 6, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[CLS] second marriage was Marie d'Agoult ( 1805 [UNK] 1876 ), who in [SEP]\""
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#print(all_labels)\n",
    "print(f\"{tokenized_data[3]['tokens']} \\n {tokenized_data[3]['ner_tags']}\")\n",
    "tokenizer.decode(tokenized_data[3]['input_ids'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

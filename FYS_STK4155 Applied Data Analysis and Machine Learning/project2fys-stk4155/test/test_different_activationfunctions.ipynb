{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e451188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import autograd.numpy as np\n",
    "\n",
    "class Scheduler:\n",
    "    \"\"\"\n",
    "    Abstract class for Schedulers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta):\n",
    "        self.eta = eta\n",
    "\n",
    "    # should be overwritten\n",
    "    def update_change(self, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # overwritten if needed\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Constant(Scheduler):\n",
    "    def __init__(self, eta):\n",
    "        super().__init__(eta)\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        return self.eta * gradient\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Momentum(Scheduler):\n",
    "    def __init__(self, eta: float, momentum: float):\n",
    "        super().__init__(eta)\n",
    "        self.momentum = momentum\n",
    "        self.change = 0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        self.change = self.momentum * self.change + self.eta * gradient\n",
    "        return self.change\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Adagrad(Scheduler):\n",
    "    def __init__(self, eta):\n",
    "        super().__init__(eta)\n",
    "        self.G_t = None\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        if self.G_t is None:\n",
    "            self.G_t = npf.zeros((gradient.shape[0], gradient.shape[0]))\n",
    "\n",
    "        self.G_t += gradient @ gradient.T\n",
    "\n",
    "        G_t_inverse = 1 / (\n",
    "            delta + npf.sqrt(npf.reshape(npf.diagonal(self.G_t), (self.G_t.shape[0], 1)))\n",
    "        )\n",
    "        return self.eta * gradient * G_t_inverse\n",
    "\n",
    "    def reset(self):\n",
    "        self.G_t = None\n",
    "\n",
    "\n",
    "class AdagradMomentum(Scheduler):\n",
    "    def __init__(self, eta, momentum):\n",
    "        super().__init__(eta)\n",
    "        self.G_t = None\n",
    "        self.momentum = momentum\n",
    "        self.change = 0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        if self.G_t is None:\n",
    "            self.G_t = npf.zeros((gradient.shape[0], gradient.shape[0]))\n",
    "\n",
    "        self.G_t += gradient @ gradient.T\n",
    "\n",
    "        G_t_inverse = 1 / (\n",
    "            delta + npf.sqrt(npf.reshape(npf.diagonal(self.G_t), (self.G_t.shape[0], 1)))\n",
    "        )\n",
    "        self.change = self.change * self.momentum + self.eta * gradient * G_t_inverse\n",
    "        return self.change\n",
    "\n",
    "    def reset(self):\n",
    "        self.G_t = None\n",
    "\n",
    "\n",
    "class RMS_prop(Scheduler):\n",
    "    def __init__(self, eta, rho):\n",
    "        super().__init__(eta)\n",
    "        self.rho = rho\n",
    "        self.second = 0.0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "        self.second = self.rho * self.second + (1 - self.rho) * gradient * gradient\n",
    "        return self.eta * gradient / (npf.sqrt(self.second + delta))\n",
    "\n",
    "    def reset(self):\n",
    "        self.second = 0.0\n",
    "\n",
    "\n",
    "class Adam(Scheduler):\n",
    "    def __init__(self, eta, rho, rho2):\n",
    "        super().__init__(eta)\n",
    "        self.rho = rho\n",
    "        self.rho2 = rho2\n",
    "        self.moment = 0\n",
    "        self.second = 0\n",
    "        self.n_epochs = 1\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        self.moment = self.rho * self.moment + (1 - self.rho) * gradient\n",
    "        self.second = self.rho2 * self.second + (1 - self.rho2) * gradient * gradient\n",
    "\n",
    "        moment_corrected = self.moment / (1 - self.rho**self.n_epochs)\n",
    "        second_corrected = self.second / (1 - self.rho2**self.n_epochs)\n",
    "\n",
    "        return self.eta * moment_corrected / (npf.sqrt(second_corrected + delta))\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_epochs += 1\n",
    "        self.moment = 0\n",
    "        self.second = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb4853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import autograd.numpy as np\n",
    "\n",
    "def CostOLS(target):\n",
    "    \n",
    "    def func(X):\n",
    "        return (1.0 / target.shape[0]) * npf.sum((target - X) ** 2)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def CostLogReg(target):\n",
    "\n",
    "    def func(X):\n",
    "        epsilon = 1e-10\n",
    "        # Apply epsilon to prevent log(0) or log(1) which results in numerical instability\n",
    "        X = npf.clip(X, epsilon, 1 - epsilon)\n",
    "        return -(1.0 / target.shape[0]) * npf.sum(\n",
    "            (target * npf.log(X)) + ((1 - target) * npf.log(1 - X))\n",
    "            )      \n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def CostCrossEntropy(target):\n",
    "    \n",
    "    def func(X):\n",
    "        return -(1.0 / target.size) * npf.sum(target * npf.log(X + 10e-10))\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c2acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import autograd.numpy as np\n",
    "#from autograd import elementwise_grad\n",
    "\n",
    "def identity(X):\n",
    "    return X\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    try:\n",
    "        return 1.0 / (1 + npf.exp(-X))\n",
    "    except FloatingPointError:\n",
    "        return npf.where(X > npf.zeros(X.shape), npf.ones(X.shape), npf.zeros(X.shape))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    X = X - npf.max(X, axis=-1, keepdims=True)\n",
    "    delta = 10e-10\n",
    "    return npf.exp(X) / (npf.sum(npf.exp(X), axis=-1, keepdims=True) + delta)\n",
    "\n",
    "\n",
    "def RELU(X):\n",
    "    return npf.where(X > npf.zeros(X.shape), X, npf.zeros(X.shape))\n",
    "\n",
    "\n",
    "def LRELU(X):\n",
    "    delta = 10e-4\n",
    "    return npf.where(X > npf.zeros(X.shape), X, delta * X)\n",
    "\n",
    "\n",
    "def derivate(func):\n",
    "    if func.__name__ == \"RELU\":\n",
    "\n",
    "        def func(X):\n",
    "            return npf.where(X > 0, 1, 0)\n",
    "\n",
    "        return func\n",
    "\n",
    "    elif func.__name__ == \"LRELU\":\n",
    "\n",
    "        def func(X):\n",
    "            delta = 10e-4\n",
    "            return npf.where(X > 0, 1, delta)\n",
    "\n",
    "        return func\n",
    "\n",
    "    else:\n",
    "        return elementwise_grad(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a407010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import autograd.numpy as npf\n",
    "import sys\n",
    "import warnings\n",
    "from autograd import grad, elementwise_grad\n",
    "from random import random, seed\n",
    "from copy import deepcopy, copy\n",
    "from typing import Tuple, Callable\n",
    "from sklearn.utils import resample\n",
    "\n",
    "warnings.simplefilter(\"error\")\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ------------\n",
    "    https://github.com/CompPhysics/MachineLearning/blob/master/doc/LectureNotes/week43.ipynb\n",
    "        \n",
    "        Feed Forward Neural Network with interface enabling flexible design of a\n",
    "        nerual networks architecture and the specification of activation function\n",
    "        in the hidden layers and output layer respectively. This model can be used\n",
    "        for both regression and classification problems, depending on the output function.\n",
    "\n",
    "    Attributes:\n",
    "    ------------\n",
    "        I   dimensions (tuple[int]): A list of positive integers, which specifies the\n",
    "            number of nodes in each of the networks layers. The first integer in the array\n",
    "            defines the number of nodes in the input layer, the second integer defines number\n",
    "            of nodes in the first hidden layer and so on until the last number, which\n",
    "            specifies the number of nodes in the output layer.\n",
    "        II  hidden_func (Callable): The activation function for the hidden layers\n",
    "        III output_func (Callable): The activation function for the output layer\n",
    "        IV  cost_func (Callable): Our cost function\n",
    "        V   seed (int): Sets random seed, makes results reproducible\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions: tuple[int],\n",
    "        hidden_func: Callable = sigmoid,\n",
    "        output_func: Callable = lambda x: x,\n",
    "        cost_func: Callable = CostOLS,\n",
    "        seed: int = None,\n",
    "    ):\n",
    "        self.dimensions = dimensions\n",
    "        self.hidden_func = hidden_func\n",
    "        self.output_func = output_func\n",
    "        self.cost_func = cost_func\n",
    "        self.seed = seed\n",
    "        self.weights = list()\n",
    "        self.schedulers_weight = list()\n",
    "        self.schedulers_bias = list()\n",
    "        self.a_matrices = list()\n",
    "        self.z_matrices = list()\n",
    "        self.classification = None\n",
    "\n",
    "        self.reset_weights()\n",
    "        self._set_classification()\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: npf.ndarray,\n",
    "        t: npf.ndarray,\n",
    "        scheduler: Scheduler,\n",
    "        batches: int = 1,\n",
    "        epochs: int = 100,\n",
    "        lam: float = 0,\n",
    "        X_val: npf.ndarray = None,\n",
    "        t_val: npf.ndarray = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            This function performs the training the neural network by performing the feedforward and backpropagation\n",
    "            algorithm to update the networks weights.\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I    X (np.ndarray) : training data\n",
    "            II   t (np.ndarray) : target data\n",
    "            III  scheduler (Scheduler) : specified scheduler (algorithm for optimization of gradient descent)\n",
    "            IV   scheduler_args (list[int]) : list of all arguments necessary for scheduler\n",
    "\n",
    "        Optional Parameters:\n",
    "        ------------\n",
    "            V    batches (int) : number of batches the datasets are split into, default equal to 1\n",
    "            VI   epochs (int) : number of iterations used to train the network, default equal to 100\n",
    "            VII  lam (float) : regularization hyperparameter lambda\n",
    "            VIII X_val (np.ndarray) : validation set\n",
    "            IX   t_val (np.ndarray) : validation target set\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            I   scores (dict) : A dictionary containing the performance metrics of the model.\n",
    "                The number of the metrics depends on the parameters passed to the fit-function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # setup \n",
    "        if self.seed is not None:\n",
    "            npf.random.seed(self.seed)\n",
    "\n",
    "        val_set = False\n",
    "        if X_val is not None and t_val is not None:\n",
    "            val_set = True\n",
    "\n",
    "        # creating arrays for score metrics\n",
    "        train_errors = npf.empty(epochs)\n",
    "        train_errors.fill(npf.nan)\n",
    "        val_errors = npf.empty(epochs)\n",
    "        val_errors.fill(npf.nan)\n",
    "\n",
    "        train_accs = npf.empty(epochs)\n",
    "        train_accs.fill(npf.nan)\n",
    "        val_accs = npf.empty(epochs)\n",
    "        val_accs.fill(npf.nan)\n",
    "\n",
    "        self.schedulers_weight = list()\n",
    "        self.schedulers_bias = list()\n",
    "\n",
    "        batch_size = X.shape[0] // batches\n",
    "\n",
    "        X, t = resample(X, t)\n",
    "\n",
    "        # this function returns a function valued only at X\n",
    "        cost_function_train = self.cost_func(t)\n",
    "        if val_set:\n",
    "            cost_function_val = self.cost_func(t_val)\n",
    "\n",
    "        # create schedulers for each weight matrix\n",
    "        for i in range(len(self.weights)):\n",
    "            self.schedulers_weight.append(copy(scheduler))\n",
    "            self.schedulers_bias.append(copy(scheduler))\n",
    "\n",
    "        print(f\"{scheduler.__class__.__name__}: Eta={scheduler.eta}, Lambda={lam}\")\n",
    "\n",
    "        try:\n",
    "            for e in range(epochs):\n",
    "                for i in range(batches):\n",
    "                    # allows for minibatch gradient descent\n",
    "                    if i == batches - 1:\n",
    "                        # If the for loop has reached the last batch, take all thats left\n",
    "                        X_batch = X[i * batch_size :, :]\n",
    "                        t_batch = t[i * batch_size :, :]\n",
    "                    else:\n",
    "                        X_batch = X[i * batch_size : (i + 1) * batch_size, :]\n",
    "                        t_batch = t[i * batch_size : (i + 1) * batch_size, :]\n",
    "\n",
    "                    self._feedforward(X_batch)\n",
    "                    self._backpropagate(X_batch, t_batch, lam)\n",
    "\n",
    "                # reset schedulers for each epoch (some schedulers pass in this call)\n",
    "                for scheduler in self.schedulers_weight:\n",
    "                    scheduler.reset()\n",
    "\n",
    "                for scheduler in self.schedulers_bias:\n",
    "                    scheduler.reset()\n",
    "\n",
    "                # computing performance metrics\n",
    "                pred_train = self.predict(X)\n",
    "                train_error = cost_function_train(pred_train)\n",
    "\n",
    "                train_errors[e] = train_error\n",
    "                if val_set:\n",
    "                    \n",
    "                    pred_val = self.predict(X_val)\n",
    "                    val_error = cost_function_val(pred_val)\n",
    "                    val_errors[e] = val_error\n",
    "\n",
    "                if self.classification:\n",
    "                    train_acc = self._accuracy(self.predict(X), t)\n",
    "                    train_accs[e] = train_acc\n",
    "                    if val_set:\n",
    "                        val_acc = self._accuracy(pred_val, t_val)\n",
    "                        val_accs[e] = val_acc\n",
    "\n",
    "                # printing progress bar\n",
    "                progression = e / epochs\n",
    "                print_length = self._progress_bar(\n",
    "                    progression,\n",
    "                    train_error=train_errors[e],\n",
    "                    train_acc=train_accs[e],\n",
    "                    val_error=val_errors[e],\n",
    "                    val_acc=val_accs[e],\n",
    "                )\n",
    "        except KeyboardInterrupt:\n",
    "            # allows for stopping training at any point and seeing the result\n",
    "            pass\n",
    "\n",
    "        # visualization of training progression (similiar to tensorflow progression bar)\n",
    "        sys.stdout.write(\"\\r\" + \" \" * print_length)\n",
    "        sys.stdout.flush()\n",
    "        self._progress_bar(\n",
    "            1,\n",
    "            train_error=train_errors[e],\n",
    "            train_acc=train_accs[e],\n",
    "            val_error=val_errors[e],\n",
    "            val_acc=val_accs[e],\n",
    "        )\n",
    "        sys.stdout.write(\"\")\n",
    "\n",
    "        # return performance metrics for the entire run\n",
    "        scores = dict()\n",
    "\n",
    "        scores[\"train_errors\"] = train_errors\n",
    "\n",
    "        if val_set:\n",
    "            scores[\"val_errors\"] = val_errors\n",
    "\n",
    "        if self.classification:\n",
    "            scores[\"train_accs\"] = train_accs\n",
    "\n",
    "            if val_set:\n",
    "                scores[\"val_accs\"] = val_accs\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X: npf.ndarray, *, threshold=0.5):\n",
    "        \"\"\"\n",
    "         Description:\n",
    "         ------------\n",
    "             Performs prediction after training of the network has been finished.\n",
    "\n",
    "         Parameters:\n",
    "        ------------\n",
    "             I   X (np.ndarray): The design matrix, with n rows of p features each\n",
    "\n",
    "         Optional Parameters:\n",
    "         ------------\n",
    "             II  threshold (float) : sets minimal value for a prediction to be predicted as the positive class\n",
    "                 in classification problems\n",
    "\n",
    "         Returns:\n",
    "         ------------\n",
    "             I   z (np.ndarray): A prediction vector (row) for each row in our design matrix\n",
    "                 This vector is thresholded if regression=False, meaning that classification results\n",
    "                 in a vector of 1s and 0s, while regressions in an array of decimal numbers\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        predict = self._feedforward(X)\n",
    "\n",
    "        if self.classification:\n",
    "            return npf.where(predict > threshold, 1, 0)\n",
    "        else:\n",
    "            return predict\n",
    "\n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Resets/Reinitializes the weights in order to train the network for a new problem.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            npf.random.seed(self.seed)\n",
    "\n",
    "        self.weights = list()\n",
    "        for i in range(len(self.dimensions) - 1):\n",
    "            weight_array = npf.random.randn(\n",
    "                self.dimensions[i] + 1, self.dimensions[i + 1]\n",
    "            )\n",
    "            weight_array[0, :] = npf.random.randn(self.dimensions[i + 1]) * 0.01\n",
    "\n",
    "            self.weights.append(weight_array)\n",
    "\n",
    "    def _feedforward(self, X: npf.ndarray):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Calculates the activation of each layer starting at the input and ending at the output.\n",
    "            Each following activation is calculated from a weighted sum of each of the preceeding\n",
    "            activations (except in the case of the input layer).\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I   X (np.ndarray): The design matrix, with n rows of p features each\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            I   z (np.ndarray): A prediction vector (row) for each row in our design matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # reset matrices\n",
    "        self.a_matrices = list()\n",
    "        self.z_matrices = list()\n",
    "\n",
    "        # if X is just a vector, make it into a matrix\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1, X.shape[0]))\n",
    "\n",
    "        # Add a coloumn of zeros as the first coloumn of the design matrix, in order\n",
    "        # to add bias to our data\n",
    "        bias = npf.ones((X.shape[0], 1)) * 0.01\n",
    "        X = npf.hstack([bias, X])\n",
    "\n",
    "        # a^0, the nodes in the input layer (one a^0 for each row in X - where the\n",
    "        # exponent indicates layer number).\n",
    "        a = X\n",
    "        self.a_matrices.append(a)\n",
    "        self.z_matrices.append(a)\n",
    "\n",
    "        # The feed forward algorithm\n",
    "        for i in range(len(self.weights)):\n",
    "            if i < len(self.weights) - 1:\n",
    "                z = a @ self.weights[i]\n",
    "                self.z_matrices.append(z)\n",
    "                a = self.hidden_func(z)\n",
    "                # bias column again added to the data here\n",
    "                bias = npf.ones((a.shape[0], 1)) * 0.01\n",
    "                a = npf.hstack([bias, a])\n",
    "                self.a_matrices.append(a)\n",
    "            else:\n",
    "                try:\n",
    "                    # a^L, the nodes in our output layers\n",
    "                    z = a @ self.weights[i]\n",
    "                    a = self.output_func(z)\n",
    "                    self.a_matrices.append(a)\n",
    "                    self.z_matrices.append(z)\n",
    "                except Exception as OverflowError:\n",
    "                    print(\n",
    "                        \"OverflowError in fit() in FFNN\\nHOW TO DEBUG ERROR: Consider lowering your learning rate or scheduler specific parameters such as momentum, or check if your input values need scaling\"\n",
    "                    )\n",
    "\n",
    "        # this will be a^L\n",
    "        return a\n",
    "\n",
    "    def _backpropagate(self, X, t, lam):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Performs the backpropagation algorithm. In other words, this method\n",
    "            calculates the gradient of all the layers starting at the\n",
    "            output layer, and moving from right to left accumulates the gradient until\n",
    "            the input layer is reached. Each layers respective weights are updated while\n",
    "            the algorithm propagates backwards from the output layer (auto-differentation in reverse mode).\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I   X (np.ndarray): The design matrix, with n rows of p features each.\n",
    "            II  t (np.ndarray): The target vector, with n rows of p targets.\n",
    "            III lam (float32): regularization parameter used to punish the weights in case of overfitting\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            No return value.\n",
    "\n",
    "        \"\"\"\n",
    "        out_derivative = derivate(self.output_func)\n",
    "        hidden_derivative = derivate(self.hidden_func)\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            # delta terms for output\n",
    "            if i == len(self.weights) - 1:\n",
    "                # for multi-class classification\n",
    "                if (\n",
    "                    self.output_func.__name__ == \"softmax\"\n",
    "                ):\n",
    "                    delta_matrix = self.a_matrices[i + 1] - t\n",
    "                # for single class classification\n",
    "                else:\n",
    "                    cost_func_derivative = grad(self.cost_func(t))\n",
    "                    delta_matrix = out_derivative(\n",
    "                        self.z_matrices[i + 1]\n",
    "                    ) * cost_func_derivative(self.a_matrices[i + 1])\n",
    "\n",
    "            # delta terms for hidden layer\n",
    "            else:\n",
    "                delta_matrix = (\n",
    "                    self.weights[i + 1][1:, :] @ delta_matrix.T\n",
    "                ).T * hidden_derivative(self.z_matrices[i + 1])\n",
    "\n",
    "            # calculate gradient\n",
    "            gradient_weights = self.a_matrices[i][:, 1:].T @ delta_matrix\n",
    "            gradient_bias = npf.sum(delta_matrix, axis=0).reshape(\n",
    "                1, delta_matrix.shape[1]\n",
    "            )\n",
    "\n",
    "            # regularization term\n",
    "            gradient_weights += self.weights[i][1:, :] * lam\n",
    "\n",
    "            # use scheduler\n",
    "            update_matrix = npf.vstack(\n",
    "                [\n",
    "                    self.schedulers_bias[i].update_change(gradient_bias),\n",
    "                    self.schedulers_weight[i].update_change(gradient_weights),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # update weights and bias\n",
    "            self.weights[i] -= update_matrix\n",
    "\n",
    "    def _accuracy(self, prediction: npf.ndarray, target: npf.ndarray):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Calculates accuracy of given prediction to target\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I   prediction (np.ndarray): vector of predicitons output network\n",
    "                (1s and 0s in case of classification, and real numbers in case of regression)\n",
    "            II  target (np.ndarray): vector of true values (What the network ideally should predict)\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            A floating point number representing the percentage of correctly classified instances.\n",
    "        \"\"\"\n",
    "        assert prediction.size == target.size\n",
    "        return npf.average((target == prediction))\n",
    "    def _set_classification(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Decides if FFNN acts as classifier (True) og regressor (False),\n",
    "            sets self.classification during init()\n",
    "        \"\"\"\n",
    "        self.classification = False\n",
    "        if (\n",
    "            self.cost_func.__name__ == \"CostLogReg\"\n",
    "            or self.cost_func.__name__ == \"CostCrossEntropy\"\n",
    "        ):\n",
    "            self.classification = True\n",
    "\n",
    "    def _progress_bar(self, progression, **kwargs):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Displays progress of training\n",
    "        \"\"\"\n",
    "        print_length = 40\n",
    "        num_equals = int(progression * print_length)\n",
    "        num_not = print_length - num_equals\n",
    "        arrow = \">\" if num_equals > 0 else \"\"\n",
    "        bar = \"[\" + \"=\" * (num_equals - 1) + arrow + \"-\" * num_not + \"]\"\n",
    "        perc_print = self._format(progression * 100, decimals=5)\n",
    "        line = f\"  {bar} {perc_print}% \"\n",
    "\n",
    "        for key in kwargs:\n",
    "            if not npf.isnan(kwargs[key]):\n",
    "                value = self._format(kwargs[key], decimals=4)\n",
    "                line += f\"| {key}: {value} \"\n",
    "        sys.stdout.write(\"\\r\" + line)\n",
    "        sys.stdout.flush()\n",
    "        return len(line)\n",
    "\n",
    "    def _format(self, value, decimals=4):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Formats decimal numbers for progress bar\n",
    "        \"\"\"\n",
    "        if value > 0:\n",
    "            v = value\n",
    "        elif value < 0:\n",
    "            v = -10 * value\n",
    "        else:\n",
    "            v = 1\n",
    "        n = 1 + math.floor(math.log10(v))\n",
    "        if n >= decimals - 1:\n",
    "            return str(round(value))\n",
    "        return f\"{value:.{decimals-n-1}f}\"\n",
    "    \n",
    "    \n",
    "\n",
    "def create_X(x, y, n):\n",
    "    if len(x.shape) > 1:\n",
    "        x = npf.ravel(x)\n",
    "        y = npf.ravel(y)\n",
    "\n",
    "    N = len(x)\n",
    "    l = int((n + 1) * (n + 2) / 2)  # Number of elements in beta\n",
    "    X = npf.ones((N, l))\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        q = int((i) * (i + 1) / 2)\n",
    "        for k in range(i + 1):\n",
    "            X[:, q + k] = (x ** (i - k)) * (y**k)\n",
    "\n",
    "    return X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b76c0250-ca55-4bef-8d92-2b0d49770b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[[1.00000000e+00 3.80380380e-02 3.26701571e-01 1.44689234e-03\n",
      "  1.24270868e-02 1.06733916e-01 5.50369458e-05 4.72701999e-04\n",
      "  4.05994877e-03 3.48701381e-02]\n",
      " [1.00000000e+00 6.82682683e-01 9.72774869e-01 4.66055645e-01\n",
      "  6.64096557e-01 9.46290946e-01 3.18168118e-01 4.53367219e-01\n",
      "  6.46016442e-01 9.20528051e-01]\n",
      " [1.00000000e+00 9.00900901e-03 2.90052356e-01 8.11622433e-05\n",
      "  2.61308429e-03 8.41303692e-02 7.31191381e-07 2.35412999e-05\n",
      "  7.57931254e-04 2.44022118e-02]\n",
      " [1.00000000e+00 5.20520521e-01 6.05235602e-01 2.70941612e-01\n",
      "  3.15037551e-01 3.66310134e-01 1.41030669e-01 1.63983510e-01\n",
      "  1.90671942e-01 2.21703935e-01]\n",
      " [1.00000000e+00 4.20420420e-02 9.26701571e-01 1.76753330e-03\n",
      "  3.89604264e-02 8.58775801e-01 7.43107093e-05 1.63797588e-03\n",
      "  3.61046883e-02 7.95828884e-01]\n",
      " [1.00000000e+00 9.61961962e-01 1.52879581e-01 9.25370816e-01\n",
      "  1.47064342e-01 2.33721663e-02 8.90171526e-01 1.41470303e-01\n",
      "  2.24831350e-02 3.57312700e-03]\n",
      " [1.00000000e+00 7.72772773e-01 1.87434555e-01 5.97177758e-01\n",
      "  1.44844321e-01 3.51317124e-02 4.61482712e-01 1.11931747e-01\n",
      "  2.71488308e-02 6.58489688e-03]\n",
      " [1.00000000e+00 9.96996997e-01 5.01570681e-01 9.94003012e-01\n",
      "  5.00064462e-01 2.51573148e-01 9.91018018e-01 4.98562767e-01\n",
      "  2.50817673e-01 1.26181715e-01]\n",
      " [1.00000000e+00 8.82882883e-01 8.70157068e-01 7.79482185e-01\n",
      "  7.68246781e-01 7.57173323e-01 6.88191479e-01 6.78271933e-01\n",
      "  6.68495366e-01 6.58859719e-01]\n",
      " [1.00000000e+00 2.80280280e-01 7.68586387e-01 7.85570355e-02\n",
      "  2.15419608e-01 5.90725035e-01 2.20179879e-02 6.03778681e-02\n",
      "  1.65568578e-01 4.54023221e-01]\n",
      " [1.00000000e+00 8.41841842e-01 8.11518325e-01 7.08697687e-01\n",
      "  6.83170081e-01 6.58561991e-01 5.96611366e-01 5.75121159e-01\n",
      "  5.54405040e-01 5.34435124e-01]\n",
      " [1.00000000e+00 3.58358358e-01 8.98429319e-01 1.28420713e-01\n",
      "  3.21959656e-01 8.07175242e-01 4.60206359e-02 1.15376934e-01\n",
      "  2.89257995e-01 7.25189903e-01]\n",
      " [1.00000000e+00 9.00900901e-02 0.00000000e+00 8.11622433e-03\n",
      "  0.00000000e+00 0.00000000e+00 7.31191381e-04 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 7.86786787e-01 5.15183246e-01 6.19033448e-01\n",
      "  4.05339371e-01 2.65413777e-01 4.87047337e-01 3.18915661e-01\n",
      "  2.08824053e-01 1.36736731e-01]\n",
      " [1.00000000e+00 5.10510511e-01 6.86910995e-01 2.60620981e-01\n",
      "  3.50675283e-01 4.71846715e-01 1.33049750e-01 1.79023418e-01\n",
      "  2.40882707e-01 3.24116696e-01]\n",
      " [1.00000000e+00 7.00700701e-01 6.35602094e-01 4.90981472e-01\n",
      "  4.45366833e-01 4.03990022e-01 3.44031061e-01 3.12068852e-01\n",
      "  2.83076092e-01 2.56776904e-01]\n",
      " [1.00000000e+00 1.88188188e-01 2.05235602e-01 3.54147942e-02\n",
      "  3.86229161e-02 4.21216524e-02 6.66464595e-03 7.26837661e-03\n",
      "  7.92679744e-03 8.64486268e-03]\n",
      " [1.00000000e+00 5.27527528e-01 7.41361257e-01 2.78285292e-01\n",
      "  3.91088471e-01 5.49616513e-01 1.46803152e-01 2.06309934e-01\n",
      "  2.89937840e-01 4.07464388e-01]\n",
      " [1.00000000e+00 7.01701702e-01 7.82198953e-01 4.92385278e-01\n",
      "  5.48870336e-01 6.11835202e-01 3.45507588e-01 3.85143249e-01\n",
      "  4.29325802e-01 4.78576854e-01]\n",
      " [1.00000000e+00 5.16516517e-01 8.90052356e-02 2.66789312e-01\n",
      "  4.59726742e-02 7.92193196e-03 1.37801086e-01 2.37456456e-02\n",
      "  4.09180870e-03 7.05093421e-04]\n",
      " [1.00000000e+00 8.50850851e-01 3.52879581e-01 7.23947170e-01\n",
      "  3.00247892e-01 1.24523999e-01 6.15971066e-01 2.55466174e-01\n",
      "  1.05951350e-01 4.39419765e-02]\n",
      " [1.00000000e+00 8.66866867e-01 4.72251309e-01 7.51458165e-01\n",
      "  4.09379013e-01 2.23021299e-01 6.51414185e-01 3.54877102e-01\n",
      "  1.93329774e-01 1.05322100e-01]\n",
      " [1.00000000e+00 4.53453453e-01 6.98429319e-01 2.05620034e-01\n",
      "  3.16705187e-01 4.87803514e-01 9.32391147e-02 1.43611061e-01\n",
      "  2.21196188e-01 3.40696276e-01]\n",
      " [1.00000000e+00 6.44644645e-01 6.94240838e-01 4.15566718e-01\n",
      "  4.47538638e-01 4.81970341e-01 2.67892859e-01 2.88503386e-01\n",
      "  3.10699599e-01 3.34603493e-01]\n",
      " [1.00000000e+00 6.21621622e-01 4.90052356e-01 3.86413440e-01\n",
      "  3.04627140e-01 2.40151312e-01 2.40202949e-01 1.89362817e-01\n",
      "  1.49283248e-01 1.17686716e-01]\n",
      " [1.00000000e+00 9.71971972e-01 5.53926702e-01 9.44729514e-01\n",
      "  5.38401228e-01 3.06834791e-01 9.18250609e-01 5.23310904e-01\n",
      "  2.98234817e-01 1.69963984e-01]\n",
      " [1.00000000e+00 2.22222222e-01 8.92146597e-01 4.93827160e-02\n",
      "  1.98254799e-01 7.95925550e-01 1.09739369e-02 4.40566221e-02\n",
      "  1.76872345e-01 7.10082271e-01]\n",
      " [1.00000000e+00 7.95795796e-01 2.74345550e-01 6.33290949e-01\n",
      "  2.18323035e-01 7.52654807e-02 5.03970274e-01 1.73740553e-01\n",
      "  5.98959531e-02 2.06487497e-02]\n",
      " [1.00000000e+00 1.24124124e-01 7.35078534e-01 1.54067982e-02\n",
      "  9.12409792e-02 5.40340451e-01 1.91235533e-03 1.13252066e-02\n",
      "  6.70692852e-02 3.97192667e-01]\n",
      " [1.00000000e+00 9.72972973e-01 8.42931937e-01 9.46676406e-01\n",
      "  8.20149993e-01 7.10534251e-01 9.21090557e-01 7.97983777e-01\n",
      "  6.91330622e-01 5.98932012e-01]\n",
      " [1.00000000e+00 7.20720721e-02 8.16753927e-01 5.19438357e-03\n",
      "  5.88651479e-02 6.67086977e-01 3.74369987e-04 4.24253318e-03\n",
      "  4.80783407e-02 5.44845908e-01]\n",
      " [1.00000000e+00 1.01101101e-01 1.51832461e-01 1.02214326e-02\n",
      "  1.53504290e-02 2.30530961e-02 1.03339810e-03 1.55194527e-03\n",
      "  2.33069340e-03 3.50020831e-03]\n",
      " [1.00000000e+00 6.56656657e-01 7.53926702e-02 4.31197965e-01\n",
      "  4.95070987e-02 5.68405471e-03 2.83149014e-01 3.25091659e-02\n",
      "  3.73247236e-03 4.28536062e-04]\n",
      " [1.00000000e+00 8.75875876e-01 1.19371728e-01 7.67158550e-01\n",
      "  1.04554817e-01 1.42496094e-02 6.71935667e-01 9.15770416e-02\n",
      "  1.24808891e-02 1.70100049e-03]\n",
      " [1.00000000e+00 7.24724725e-01 6.50261780e-01 5.25225927e-01\n",
      "  4.71260790e-01 4.22840383e-01 3.80644215e-01 3.41534346e-01\n",
      "  3.06442880e-01 2.74956940e-01]\n",
      " [1.00000000e+00 7.73773774e-01 9.90575916e-01 5.98725853e-01\n",
      "  7.66481665e-01 9.81240646e-01 4.63278363e-01 5.93083410e-01\n",
      "  7.59258277e-01 9.71993352e-01]\n",
      " [1.00000000e+00 3.99399399e-01 1.82198953e-01 1.59519880e-01\n",
      "  7.27701524e-02 3.31964584e-02 6.37121444e-02 2.90643551e-02\n",
      "  1.32586456e-02 6.04835997e-03]\n",
      " [1.00000000e+00 6.00600601e-01 1.74869110e-01 3.60721081e-01\n",
      "  1.05026492e-01 3.05792056e-02 2.16649298e-01 6.30789745e-02\n",
      "  1.83658893e-02 5.34735847e-03]\n",
      " [1.00000000e+00 2.93293293e-01 6.73298429e-01 8.60209559e-02\n",
      "  1.97473914e-01 4.53330775e-01 2.52293694e-02 5.79177745e-02\n",
      "  1.32958876e-01 3.05226899e-01]\n",
      " [1.00000000e+00 1.98198198e-01 3.24607330e-02 3.92825258e-02\n",
      "  6.43365879e-03 1.05369919e-03 7.78572583e-03 1.27513958e-03\n",
      "  2.08841280e-04 3.42038479e-05]\n",
      " [1.00000000e+00 1.44144144e-01 1.75916230e-01 2.07775343e-02\n",
      "  2.53572945e-02 3.09465201e-02 2.99495990e-03 3.65510551e-03\n",
      "  4.46075965e-03 5.44399516e-03]\n",
      " [1.00000000e+00 1.30130130e-01 4.77486911e-01 1.69338508e-02\n",
      "  6.21354339e-02 2.27993750e-01 2.20360420e-03 8.08569209e-03\n",
      "  2.96688564e-02 1.08864031e-01]\n",
      " [1.00000000e+00 5.39539540e-01 6.13612565e-01 2.91102915e-01\n",
      "  3.31068241e-01 3.76520380e-01 1.57061533e-01 1.78624406e-01\n",
      "  2.03147633e-01 2.31037637e-01]\n",
      " [1.00000000e+00 6.38638639e-01 5.32984293e-01 4.07859311e-01\n",
      "  3.40384363e-01 2.84072257e-01 2.60474715e-01 2.17382606e-01\n",
      "  1.81419519e-01 1.51406051e-01]\n",
      " [1.00000000e+00 9.75975976e-01 3.32984293e-01 9.52529106e-01\n",
      "  3.24984671e-01 1.10878540e-01 9.29645524e-01 3.17177231e-01\n",
      "  1.08214791e-01 3.69208121e-02]\n",
      " [1.00000000e+00 9.47947948e-01 1.00000000e+00 8.98605312e-01\n",
      "  9.47947948e-01 1.00000000e+00 8.51831062e-01 8.98605312e-01\n",
      "  9.47947948e-01 1.00000000e+00]\n",
      " [1.00000000e+00 7.64764765e-01 4.17801047e-01 5.84865145e-01\n",
      "  3.19519520e-01 1.74557715e-01 4.47284255e-01 2.44357270e-01\n",
      "  1.33495590e-01 7.29303961e-02]\n",
      " [1.00000000e+00 8.70870871e-02 8.67015707e-01 7.58416074e-03\n",
      "  7.55058724e-02 7.51716236e-01 6.60482467e-04 6.57558648e-03\n",
      "  6.54647773e-02 6.51749784e-01]\n",
      " [1.00000000e+00 6.53653654e-01 2.03141361e-01 4.27263099e-01\n",
      "  1.32784093e-01 4.12664127e-02 2.79282086e-01 8.67948075e-02\n",
      "  2.69739414e-02 8.38291524e-03]\n",
      " [1.00000000e+00 2.91291291e-01 4.14659686e-01 8.48506164e-02\n",
      "  1.20786755e-01 1.71942655e-01 2.47162456e-02 3.51841299e-02\n",
      "  5.00853980e-02 7.12976873e-02]\n",
      " [1.00000000e+00 2.87287287e-01 3.36125654e-01 8.25339854e-02\n",
      "  9.65646275e-02 1.12980456e-01 2.37109648e-02 2.77417899e-02\n",
      "  3.24578486e-02 3.79756296e-02]\n",
      " [1.00000000e+00 1.15115115e-01 3.50785340e-01 1.32514897e-02\n",
      "  4.03806948e-02 1.23050355e-01 1.52544677e-03 4.64842833e-03\n",
      "  1.41649558e-02 4.31642606e-02]\n",
      " [1.00000000e+00 3.68368368e-01 8.25130890e-01 1.35695255e-01\n",
      "  3.03952120e-01 6.80840986e-01 4.99858396e-02 1.11966346e-01\n",
      "  2.50800283e-01 5.61782929e-01]\n",
      " [1.00000000e+00 4.98498498e-01 1.01570681e-01 2.48500753e-01\n",
      "  5.06328318e-02 1.03166032e-02 1.23877252e-01 2.52403906e-02\n",
      "  5.14281119e-03 1.04786441e-03]\n",
      " [1.00000000e+00 7.27727728e-01 4.61780105e-01 5.29587646e-01\n",
      "  3.36050186e-01 2.13240865e-01 3.85395614e-01 2.44553038e-01\n",
      "  1.55181290e-01 9.84703890e-02]\n",
      " [1.00000000e+00 7.54754755e-01 1.42408377e-01 5.69654740e-01\n",
      "  1.07483400e-01 2.02801458e-02 4.29949623e-01 8.11236069e-02\n",
      "  1.53065365e-02 2.88806265e-03]\n",
      " [1.00000000e+00 6.46646647e-01 1.21465969e-01 4.18151886e-01\n",
      "  7.85455613e-02 1.47539815e-02 2.70396515e-01 5.07912238e-02\n",
      "  9.54061268e-03 1.79210666e-03]\n",
      " [1.00000000e+00 4.80480480e-02 6.26178010e-01 2.30861492e-03\n",
      "  3.00866311e-02 3.92098901e-01 1.10924441e-04 1.44560390e-03\n",
      "  1.88395868e-02 2.45523710e-01]\n",
      " [1.00000000e+00 4.67467467e-01 1.37172775e-01 2.18525833e-01\n",
      "  6.41238097e-02 1.88163702e-02 1.02153718e-01 2.99757949e-02\n",
      "  8.79604091e-03 2.58109371e-03]\n",
      " [1.00000000e+00 0.00000000e+00 5.65445026e-01 0.00000000e+00\n",
      "  0.00000000e+00 3.19728078e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.80788651e-01]\n",
      " [1.00000000e+00 3.41341341e-01 8.94240838e-01 1.16513911e-01\n",
      "  3.05241367e-01 7.99666676e-01 3.97710148e-02 1.04191498e-01\n",
      "  2.72959296e-01 7.15094598e-01]\n",
      " [1.00000000e+00 1.64164164e-01 1.40314136e-01 2.69498728e-02\n",
      "  2.30345529e-02 1.96880568e-02 4.42420334e-03 3.78144812e-03\n",
      "  3.23207339e-03 2.76251268e-03]\n",
      " [1.00000000e+00 2.11211211e-01 2.82722513e-02 4.46101757e-02\n",
      "  5.97141644e-03 7.99320194e-04 9.42216925e-03 1.26123010e-03\n",
      "  1.68825386e-04 2.25985814e-05]\n",
      " [1.00000000e+00 2.45245245e-01 2.12565445e-01 6.01452303e-02\n",
      "  5.21306647e-02 4.51840684e-02 1.47503318e-02 1.27847976e-02\n",
      "  1.10811779e-02 9.60457161e-03]\n",
      " [1.00000000e+00 7.99799800e-01 9.86387435e-01 6.39679720e-01\n",
      "  7.88912473e-01 9.72960171e-01 5.11615712e-01 6.30972038e-01\n",
      "  7.78173350e-01 9.59715687e-01]\n",
      " [1.00000000e+00 3.06306306e-01 8.97382199e-01 9.38235533e-02\n",
      "  2.74873827e-01 8.05294811e-01 2.87387461e-02 8.41955866e-02\n",
      "  2.46666879e-01 7.22657228e-01]\n",
      " [1.00000000e+00 9.88988989e-01 7.74869110e-02 9.78099220e-01\n",
      "  7.66337018e-02 6.00422138e-03 9.67329359e-01 7.57898872e-02\n",
      "  5.93810883e-03 4.65248567e-04]\n",
      " [1.00000000e+00 3.85385385e-01 8.78534031e-01 1.48521895e-01\n",
      "  3.38574176e-01 7.71822044e-01 5.72381678e-02 1.30481539e-01\n",
      "  2.97448936e-01 6.78071932e-01]\n",
      " [1.00000000e+00 1.35135135e-01 4.91099476e-01 1.82615047e-02\n",
      "  6.63647941e-02 2.41178696e-01 2.46777091e-03 8.96821542e-03\n",
      "  3.25917156e-02 1.18442731e-01]\n",
      " [1.00000000e+00 2.69269269e-01 2.89005236e-01 7.25059394e-02\n",
      "  7.78202286e-02 8.35240262e-02 1.95236213e-02 2.09545961e-02\n",
      "  2.24904535e-02 2.41388809e-02]\n",
      " [1.00000000e+00 4.69469469e-01 3.14136126e-01 2.20401583e-01\n",
      "  1.47477320e-01 9.86815054e-02 1.03471814e-01 6.92360993e-02\n",
      "  4.63279540e-02 3.09994258e-02]\n",
      " [1.00000000e+00 1.00000000e+00 7.93717277e-01 1.00000000e+00\n",
      "  7.93717277e-01 6.29987117e-01 1.00000000e+00 7.93717277e-01\n",
      "  6.29987117e-01 5.00031659e-01]\n",
      " [1.00000000e+00 4.33433433e-01 1.94764398e-01 1.87864541e-01\n",
      "  8.44174017e-02 3.79331707e-02 8.14267731e-02 3.65893243e-02\n",
      "  1.64415044e-02 7.38803115e-03]\n",
      " [1.00000000e+00 2.56256256e-01 7.04712042e-01 6.56672689e-02\n",
      "  1.80586870e-01 4.96619062e-01 1.68276485e-02 4.62765151e-02\n",
      "  1.27261742e-01 3.49973433e-01]\n",
      " [1.00000000e+00 1.66166166e-01 3.82198953e-01 2.76111948e-02\n",
      "  6.35085347e-02 1.46076040e-01 4.58804638e-03 1.05529697e-02\n",
      "  2.42728955e-02 5.58301094e-02]\n",
      " [1.00000000e+00 3.23323323e-01 1.18324607e-01 1.04537971e-01\n",
      "  3.82571053e-02 1.40007127e-02 3.37995643e-02 1.23694144e-02\n",
      "  4.52675696e-03 1.65662883e-03]\n",
      " [1.00000000e+00 3.78378378e-01 2.94240838e-01 1.43170197e-01\n",
      "  1.11334371e-01 8.65776706e-02 5.41725071e-02 4.21265188e-02\n",
      "  3.27591186e-02 2.54746863e-02]\n",
      " [1.00000000e+00 1.79179179e-01 4.86910995e-01 3.21051783e-02\n",
      "  8.72443124e-02 2.37082317e-01 5.75257949e-03 1.56323643e-02\n",
      "  4.24802149e-02 1.15437987e-01]\n",
      " [1.00000000e+00 7.16716717e-01 6.24083770e-01 5.13682852e-01\n",
      "  4.47291270e-01 3.89480552e-01 3.68165087e-01 3.20581131e-01\n",
      "  2.79147222e-01 2.43068491e-01]\n",
      " [1.00000000e+00 1.03103103e-01 7.85340314e-02 1.06302499e-02\n",
      "  8.09710234e-03 6.16759409e-03 1.09601175e-03 8.34836377e-04\n",
      "  6.35898089e-04 4.84366028e-04]\n",
      " [1.00000000e+00 4.51451451e-01 7.17277487e-01 2.03808413e-01\n",
      "  3.23815963e-01 5.14486993e-01 9.20096039e-02 1.46187186e-01\n",
      "  2.32265900e-01 3.69029938e-01]\n",
      " [1.00000000e+00 1.80180180e-01 5.09947644e-01 3.24648973e-02\n",
      "  9.18824584e-02 2.60046600e-01 5.84953105e-03 1.65553979e-02\n",
      "  4.68552432e-02 1.32610151e-01]\n",
      " [1.00000000e+00 7.12712713e-01 4.07329843e-01 5.07959411e-01\n",
      "  2.90309157e-01 1.65917601e-01 3.62029130e-01 2.06907027e-01\n",
      "  1.18251583e-01 6.75831903e-02]\n",
      " [1.00000000e+00 6.72672673e-01 5.30890052e-01 4.52488525e-01\n",
      "  3.57115230e-01 2.81844248e-01 3.04376665e-01 2.40221656e-01\n",
      "  1.89588923e-01 1.49628307e-01]\n",
      " [1.00000000e+00 8.65865866e-01 7.29842932e-01 7.49723698e-01\n",
      "  6.31946082e-01 5.32670705e-01 6.49160159e-01 5.47180542e-01\n",
      "  4.61221381e-01 3.88765949e-01]\n",
      " [1.00000000e+00 9.67967968e-01 1.50785340e-01 9.36961987e-01\n",
      "  1.45955379e-01 2.27362189e-02 9.06949191e-01 1.41280132e-01\n",
      "  2.20079316e-02 3.42828850e-03]\n",
      " [1.00000000e+00 4.42442442e-01 1.45549738e-01 1.95755315e-01\n",
      "  6.43973817e-02 2.11847263e-02 8.66104596e-02 2.84921348e-02\n",
      "  9.37302204e-03 3.08343137e-03]\n",
      " [1.00000000e+00 4.47447447e-01 1.60209424e-01 2.00209218e-01\n",
      "  7.16852979e-02 2.56670596e-02 8.95831037e-02 3.20754035e-02\n",
      "  1.14846603e-02 4.11210483e-03]\n",
      " [1.00000000e+00 1.50150150e-01 8.17801047e-01 2.25450676e-02\n",
      "  1.22792950e-01 6.68798553e-01 3.38514528e-03 1.84373799e-02\n",
      "  1.00420203e-01 5.46944157e-01]\n",
      " [1.00000000e+00 2.37237237e-01 6.53403141e-01 5.62815067e-02\n",
      "  1.55011556e-01 4.26935665e-01 1.33520692e-02 3.67745133e-02\n",
      "  1.01285038e-01 2.78961105e-01]\n",
      " [1.00000000e+00 2.05205205e-01 6.61780105e-01 4.21091762e-02\n",
      "  1.35800722e-01 4.37952907e-01 8.64102215e-03 2.78670151e-02\n",
      "  8.98702161e-02 2.89828521e-01]\n",
      " [1.00000000e+00 5.69569570e-01 2.48167539e-01 3.24409495e-01\n",
      "  1.41348679e-01 6.15871275e-02 1.84773776e-01 8.05079060e-02\n",
      "  3.50781537e-02 1.52839259e-02]\n",
      " [1.00000000e+00 4.29429429e-01 6.93193717e-01 1.84409635e-01\n",
      "  2.97677782e-01 4.80517530e-01 7.91909243e-02 1.27831600e-01\n",
      "  2.06348369e-01 3.33091733e-01]\n",
      " [1.00000000e+00 4.89489489e-01 4.58638743e-01 2.39599960e-01\n",
      "  2.24498844e-01 2.10349497e-01 1.17281662e-01 1.09889825e-01\n",
      "  1.02963868e-01 9.64744290e-02]\n",
      " [1.00000000e+00 4.60460460e-01 9.97905759e-01 2.12023836e-01\n",
      "  4.59496145e-01 9.95815904e-01 9.76285930e-02 2.11579807e-01\n",
      "  4.58533850e-01 9.93730426e-01]\n",
      " [1.00000000e+00 6.08608609e-01 8.68062827e-01 3.70404438e-01\n",
      "  5.28310509e-01 7.53533072e-01 2.25431330e-01 3.21534324e-01\n",
      "  4.58606714e-01 6.54114049e-01]\n",
      " [1.00000000e+00 3.45345345e-01 9.88481675e-01 1.19263408e-01\n",
      "  3.41367546e-01 9.77096023e-01 4.11870627e-02 1.17889693e-01\n",
      "  3.37435563e-01 9.65841513e-01]\n",
      " [1.00000000e+00 2.07207207e-01 6.31413613e-01 4.29348267e-02\n",
      "  1.30833451e-01 3.98683150e-01 8.89640554e-03 2.71096340e-02\n",
      "  8.26100221e-02 2.51733968e-01]\n",
      " [1.00000000e+00 3.89389389e-01 5.35078534e-01 1.51624097e-01\n",
      "  2.08353904e-01 2.86309038e-01 5.90408144e-02 8.11307993e-02\n",
      "  1.11485701e-01 1.53197820e-01]\n",
      " [1.00000000e+00 9.24924925e-01 9.52879581e-01 8.55486117e-01\n",
      "  8.81342075e-01 9.07979496e-01 7.91260432e-01 8.15175253e-01\n",
      "  8.39812867e-01 8.65195122e-01]]\n"
     ]
    }
   ],
   "source": [
    "#This code is used to compute all training,test,bootrap,cross valdation MSE for all models variyng both polynomial\n",
    "#degree and tunining paramter for Ridge and Lasso.\n",
    "\n",
    "\n",
    "#import packages\n",
    "#import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from random import random, seed\n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error,mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures,MinMaxScaler\n",
    "import math\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import imageio \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoLars\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "#set seed\n",
    "npf.random.seed(1244)\n",
    "\n",
    "#Import geographical image\n",
    "terrain = imageio.v2.imread('SRTM_data_Norway_1.tif')\n",
    "N = 1000\n",
    "\n",
    "# Descretize the image\n",
    "terrain = terrain[:N, :N]\n",
    "\n",
    "\n",
    "#Sample 100 points from the image bu sampling x and y coordiante and then extracting the image pixel value \n",
    "sample_size=100\n",
    "row = npf.random.choice(N ,sample_size, replace=False)\n",
    "column=npf.random.choice(N , sample_size, replace=False)\n",
    "\n",
    "\n",
    "samp=npf.vstack([row,column]).T\n",
    "\n",
    "\n",
    "z=terrain[samp[:,0],samp[:,1]] \n",
    "\n",
    "#Min-Max scale on x,y and z\n",
    "row=row.reshape(-1, 1)\n",
    "scaler1 =MinMaxScaler()\n",
    "scaler1.fit(row)\n",
    "row=scaler1.transform(row).ravel()\n",
    "        \n",
    "column=column.reshape(-1, 1)\n",
    "scaler2 =MinMaxScaler()\n",
    "scaler2.fit(column)\n",
    "column=scaler2.transform(column).ravel()\n",
    "        \n",
    "\n",
    "z=z.reshape(-1, 1)\n",
    "scaler3 =MinMaxScaler()\n",
    "scaler3.fit(z)\n",
    "z=scaler3.transform(z).ravel()+0.5*npf.random.rand(sample_size)\n",
    "print(z.shape)       \n",
    "z = z.reshape(z.shape[0], 1)\n",
    "poly_degree=3\n",
    "X = create_X(row, column, poly_degree)\n",
    "X_train,X_test,t_train,t_test=train_test_split(X,z,test_size=0.2)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fda6b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant: Eta=0.001, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 0.0864 Constant: Eta=0.001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0319 "
     ]
    }
   ],
   "source": [
    "input_nodes = X_train.shape[1]\n",
    "output_nodes = 1\n",
    "\n",
    "linear_regression = FFNN((input_nodes,64,64,64,64,64, output_nodes), output_func=identity, cost_func=CostOLS, seed=2023)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scheduler = Constant(eta=1e-3)\n",
    "scores = linear_regression.fit(X_train, t_train, scheduler)\n",
    "\n",
    "linear_regression.reset_weights() # reset weights such that previous runs or reruns don't affect the weights\n",
    "\n",
    "scores = linear_regression.fit(X_train, t_train, scheduler, lam=1e-4, epochs=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c27bf726-102a-4fc0-9ff3-70e12575ee4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05473591405940085"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "#for i in range(X_test.shape[1]):\n",
    "y_hat=linear_regression.predict( X_test)\n",
    "\n",
    "mean_squared_error(t_test,y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c42db4a3-eac1-4645-bc4d-5a5c15766f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Eta=0.0001, Lambda=1e-05\n",
      "  [=======================================>] 100.0% | train_error: 29.7 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0206 Adam: Eta=0.0001, Lambda=1e-05\n",
      "  [=======================================>] 100.0% | train_error: 1.85 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0150 Adam: Eta=0.0001, Lambda=1e-05\n",
      "  [=======================================>] 100.0% | train_error: 0.221 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0118 Adam: Eta=0.0001, Lambda=1e-05\n",
      "  [=======================================>] 100.0% | train_error: 0.0656 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0116 Adam: Eta=0.0001, Lambda=1e-05\n",
      "  [=======================================>] 100.0% | train_error: 0.429 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0123 Adam: Eta=0.0001, Lambda=2e-05\n",
      "  [=======================================>] 100.0% | train_error: 29.7 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0206 Adam: Eta=0.0001, Lambda=2e-05\n",
      "  [=======================================>] 100.0% | train_error: 1.85 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0150 Adam: Eta=0.0001, Lambda=2e-05\n",
      "  [=======================================>] 100.0% | train_error: 0.221 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0118 Adam: Eta=0.0001, Lambda=2e-05\n",
      "  [=======================================>] 100.0% | train_error: 0.0656 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0116 Adam: Eta=0.0001, Lambda=2e-05\n",
      "  [=======================================>] 100.0% | train_error: 0.429 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0123 Adam: Eta=0.0001, Lambda=4e-05\n",
      "  [=======================================>] 100.0% | train_error: 29.7 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0206 Adam: Eta=0.0001, Lambda=4e-05\n",
      "  [=======================================>] 100.0% | train_error: 1.85 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0150 Adam: Eta=0.0001, Lambda=4e-05\n",
      "  [=======================================>] 100.0% | train_error: 0.221 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0118 Adam: Eta=0.0001, Lambda=4e-05\n",
      "  [=======================================>] 100.0% | train_error: 0.0656 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0116 Adam: Eta=0.0001, Lambda=4e-05\n",
      "  [=======================================>] 100.0% | train_error: 0.429 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0123 Adam: Eta=0.0001, Lambda=0.0005\n",
      "  [=======================================>] 100.0% | train_error: 29.7 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0206 Adam: Eta=0.0001, Lambda=0.0005\n",
      "  [=======================================>] 100.0% | train_error: 1.85 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0150 Adam: Eta=0.0001, Lambda=0.0005\n",
      "  [=======================================>] 100.0% | train_error: 0.223 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0118 Adam: Eta=0.0001, Lambda=0.0005\n",
      "  [=======================================>] 100.0% | train_error: 0.0687 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0116 Adam: Eta=0.0001, Lambda=0.0005\n",
      "  [=======================================>] 100.0% | train_error: 0.436 Adam: Eta=0.0001, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 0.0123 Adam: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 14336764 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 14336261  Adam: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 20610698 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 20609637  Adam: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 11184887 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 11183840  Adam: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 272495130 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 272480567  Adam: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 1470779632 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 1470704156  Adam: Eta=1e-10, Lambda=1e-05\n",
      "  [=======================================>] 100.0% | train_error: 14263948 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 14263446  Adam: Eta=1e-10, Lambda=1e-05\n",
      "  [=======================================>] 100.0% | train_error: 20673668 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 20672606  Adam: Eta=1e-10, Lambda=1e-05\n",
      "  [=======================================>] 100.0% | train_error: 11278594 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 11277541  Adam: Eta=1e-10, Lambda=1e-05\n",
      "  [=======================================>] 100.0% | train_error: 271551550 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 271537029  Adam: Eta=1e-10, Lambda=1e-05\n",
      "  [=======================================>] 100.0% | train_error: 1469175759 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 1469100101  Adam: Eta=1e-10, Lambda=2e-05\n",
      "  [=======================================>] 100.0% | train_error: 14263948 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 14263446  Adam: Eta=1e-10, Lambda=2e-05\n",
      "  [=======================================>] 100.0% | train_error: 20673668 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 20672606  Adam: Eta=1e-10, Lambda=2e-05\n",
      "  [=======================================>] 100.0% | train_error: 11278594 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 11277541  Adam: Eta=1e-10, Lambda=2e-05\n",
      "  [=======================================>] 100.0% | train_error: 271551550 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 271537029  Adam: Eta=1e-10, Lambda=2e-05\n",
      "  [=======================================>] 100.0% | train_error: 1469175759 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 1469100101  Adam: Eta=1e-10, Lambda=4e-05\n",
      "  [=======================================>] 100.0% | train_error: 14263948 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 14263446  Adam: Eta=1e-10, Lambda=4e-05\n",
      "  [=======================================>] 100.0% | train_error: 20673668 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 20672606  Adam: Eta=1e-10, Lambda=4e-05\n",
      "  [=======================================>] 100.0% | train_error: 11278594 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 11277541  Adam: Eta=1e-10, Lambda=4e-05\n",
      "  [=======================================>] 100.0% | train_error: 271551550 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 271537029  Adam: Eta=1e-10, Lambda=4e-05\n",
      "  [=======================================>] 100.0% | train_error: 1469175759 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 1469100101  Adam: Eta=1e-10, Lambda=0.0005\n",
      "  [=======================================>] 100.0% | train_error: 14263948 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 14263446  Adam: Eta=1e-10, Lambda=0.0005\n",
      "  [=======================================>] 100.0% | train_error: 20673668 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 20672606  Adam: Eta=1e-10, Lambda=0.0005\n",
      "  [=======================================>] 100.0% | train_error: 11278594 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 11277541  Adam: Eta=1e-10, Lambda=0.0005\n",
      "  [=======================================>] 100.0% | train_error: 271551550 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 271537029  Adam: Eta=1e-10, Lambda=0.0005\n",
      "  [=======================================>] 100.0% | train_error: 1469175759 Adam: Eta=1e-10, Lambda=0.0001\n",
      "  [=======================================>] 100.0% | train_error: 1469100101  "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>degree</th>\n",
       "      <th>mse_train</th>\n",
       "      <th>mse_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>0.034506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031164</td>\n",
       "      <td>0.031164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.090950</td>\n",
       "      <td>0.090950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.063844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.151807</td>\n",
       "      <td>0.151807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>0.034506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031164</td>\n",
       "      <td>0.031164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>3</td>\n",
       "      <td>0.090950</td>\n",
       "      <td>0.090950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.063844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>5</td>\n",
       "      <td>0.151807</td>\n",
       "      <td>0.151807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>0.034506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031164</td>\n",
       "      <td>0.031164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>3</td>\n",
       "      <td>0.090950</td>\n",
       "      <td>0.090950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.063844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>5</td>\n",
       "      <td>0.151807</td>\n",
       "      <td>0.151807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>0.034506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031164</td>\n",
       "      <td>0.031164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>3</td>\n",
       "      <td>0.090950</td>\n",
       "      <td>0.090950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.063844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>5</td>\n",
       "      <td>0.151807</td>\n",
       "      <td>0.151807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha  degree  mse_train  mse_test\n",
       "0   0.00001       1   0.034506  0.034506\n",
       "1   0.00001       2   0.031164  0.031164\n",
       "2   0.00001       3   0.090950  0.090950\n",
       "3   0.00001       4   0.063844  0.063844\n",
       "4   0.00001       5   0.151807  0.151807\n",
       "5   0.00002       1   0.034506  0.034506\n",
       "6   0.00002       2   0.031164  0.031164\n",
       "7   0.00002       3   0.090950  0.090950\n",
       "8   0.00002       4   0.063844  0.063844\n",
       "9   0.00002       5   0.151807  0.151807\n",
       "10  0.00004       1   0.034506  0.034506\n",
       "11  0.00004       2   0.031164  0.031164\n",
       "12  0.00004       3   0.090950  0.090950\n",
       "13  0.00004       4   0.063844  0.063844\n",
       "14  0.00004       5   0.151807  0.151807\n",
       "15  0.00050       1   0.034506  0.034506\n",
       "16  0.00050       2   0.031164  0.031164\n",
       "17  0.00050       3   0.090950  0.090950\n",
       "18  0.00050       4   0.063844  0.063844\n",
       "19  0.00050       5   0.151807  0.151807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>degree</th>\n",
       "      <th>mse_train</th>\n",
       "      <th>mse_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.490930e+07</td>\n",
       "      <td>1.490930e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.452181e+07</td>\n",
       "      <td>2.452181e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.411978e+07</td>\n",
       "      <td>1.411978e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.600550e+08</td>\n",
       "      <td>4.600550e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.824034e+09</td>\n",
       "      <td>1.824034e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  degree     mse_train      mse_test\n",
       "0      0       1  1.490930e+07  1.490930e+07\n",
       "1      0       2  2.452181e+07  2.452181e+07\n",
       "2      0       3  1.411978e+07  1.411978e+07\n",
       "3      0       4  4.600550e+08  4.600550e+08\n",
       "4      0       5  1.824034e+09  1.824034e+09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>degree</th>\n",
       "      <th>mse_train</th>\n",
       "      <th>mse_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>1.483754e+07</td>\n",
       "      <td>1.483754e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>2.459653e+07</td>\n",
       "      <td>2.459653e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>3</td>\n",
       "      <td>1.422732e+07</td>\n",
       "      <td>1.422732e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>4.584313e+08</td>\n",
       "      <td>4.584313e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>5</td>\n",
       "      <td>1.822756e+09</td>\n",
       "      <td>1.822756e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>1</td>\n",
       "      <td>1.483754e+07</td>\n",
       "      <td>1.483754e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>2</td>\n",
       "      <td>2.459653e+07</td>\n",
       "      <td>2.459653e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>3</td>\n",
       "      <td>1.422732e+07</td>\n",
       "      <td>1.422732e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>4</td>\n",
       "      <td>4.584313e+08</td>\n",
       "      <td>4.584313e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>5</td>\n",
       "      <td>1.822756e+09</td>\n",
       "      <td>1.822756e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>1</td>\n",
       "      <td>1.483754e+07</td>\n",
       "      <td>1.483754e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>2</td>\n",
       "      <td>2.459653e+07</td>\n",
       "      <td>2.459653e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>3</td>\n",
       "      <td>1.422732e+07</td>\n",
       "      <td>1.422732e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>4</td>\n",
       "      <td>4.584313e+08</td>\n",
       "      <td>4.584313e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>5</td>\n",
       "      <td>1.822756e+09</td>\n",
       "      <td>1.822756e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>1</td>\n",
       "      <td>1.483754e+07</td>\n",
       "      <td>1.483754e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>2</td>\n",
       "      <td>2.459653e+07</td>\n",
       "      <td>2.459653e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>3</td>\n",
       "      <td>1.422732e+07</td>\n",
       "      <td>1.422732e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>4</td>\n",
       "      <td>4.584313e+08</td>\n",
       "      <td>4.584313e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>5</td>\n",
       "      <td>1.822756e+09</td>\n",
       "      <td>1.822756e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha  degree     mse_train      mse_test\n",
       "0   0.00001       1  1.483754e+07  1.483754e+07\n",
       "1   0.00001       2  2.459653e+07  2.459653e+07\n",
       "2   0.00001       3  1.422732e+07  1.422732e+07\n",
       "3   0.00001       4  4.584313e+08  4.584313e+08\n",
       "4   0.00001       5  1.822756e+09  1.822756e+09\n",
       "5   0.00002       1  1.483754e+07  1.483754e+07\n",
       "6   0.00002       2  2.459653e+07  2.459653e+07\n",
       "7   0.00002       3  1.422732e+07  1.422732e+07\n",
       "8   0.00002       4  4.584313e+08  4.584313e+08\n",
       "9   0.00002       5  1.822756e+09  1.822756e+09\n",
       "10  0.00004       1  1.483754e+07  1.483754e+07\n",
       "11  0.00004       2  2.459653e+07  2.459653e+07\n",
       "12  0.00004       3  1.422732e+07  1.422732e+07\n",
       "13  0.00004       4  4.584313e+08  4.584313e+08\n",
       "14  0.00004       5  1.822756e+09  1.822756e+09\n",
       "15  0.00050       1  1.483754e+07  1.483754e+07\n",
       "16  0.00050       2  2.459653e+07  2.459653e+07\n",
       "17  0.00050       3  1.422732e+07  1.422732e+07\n",
       "18  0.00050       4  4.584313e+08  4.584313e+08\n",
       "19  0.00050       5  1.822756e+09  1.822756e+09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "t_scale=npf.mean(t_train)\n",
    "# Bootstrap and KFold class definitions\n",
    "class BootstrapKfold:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def bootstrap(self, n, function, degree, z, X_train, X_test, y_train, y_test, alpha):\n",
    "        \"\"\"\n",
    "        Perform bootstrap resampling and calculate Cross-Validated Mean Squared Errors (MSE) for regression models.\n",
    "\n",
    "        Parameters:\n",
    "            n (int): Number of bootstrap samples.\n",
    "            function (function): Regression function to be evaluated.\n",
    "            degree (int): Polynomial degree for regression.\n",
    "            z (array): Target values for regression.\n",
    "            X_train (array): Training features.\n",
    "            X_test (array): Testing features.\n",
    "            y_train (array): Training target values.\n",
    "            y_test (array): Testing target values.\n",
    "            alpha (float): Regularization parameter (alpha) for Ridge and Lasso regression.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-validated mean training MSE.\n",
    "            float: Cross-validated mean test MSE.\n",
    "        \"\"\"\n",
    "        \n",
    "        Boot_test_MSE = 0\n",
    "        Boot_train_MSE = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            indices = npf.random.choice(X_train.shape[0], size=z.shape[0], replace=True)\n",
    "            X_boot = X_train[indices]\n",
    "            y_boot = y_train[indices]\n",
    "\n",
    "            train_MSE, test_MSE = function(degree, X_boot, X_test, y_boot, y_test, alpha)\n",
    "\n",
    "            Boot_test_MSE += test_MSE\n",
    "            Boot_train_MSE += train_MSE\n",
    "\n",
    "        Boot_test_MSE /= n\n",
    "        Boot_train_MSE /= n\n",
    "\n",
    "        return Boot_train_MSE, Boot_test_MSE\n",
    "\n",
    "    def kfold(self, K, function, degree, z,X,alpha):\n",
    "        \"\"\"\n",
    "        Perform K-Fold cross-validation and calculate Mean Squared Errors (MSE) for regression models.\n",
    "\n",
    "        Parameters:\n",
    "            K (int): Number of folds.\n",
    "            function (function): Regression function to be evaluated.\n",
    "            degree (int): Polynomial degree for regression.\n",
    "            z (array): Target values for regression.\n",
    "            X_train (array): Training features.\n",
    "            X_test (array): Testing features.\n",
    "            y_train (array): Training target values.\n",
    "            y_test (array): Testing target values.\n",
    "            alpha (float): Regularization parameter (alpha) for Ridge and Lasso regression.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-validated mean training MSE.\n",
    "            float: Cross-validated mean test MSE.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "       \n",
    "      \n",
    "\n",
    "        #shuffle data\n",
    "        X=shuffle(X, random_state=0)\n",
    "        z=shuffle(z, random_state=0)\n",
    "        \n",
    "        #indexes for k-fold boundaries\n",
    "        fold_size = int(math.ceil(len(z) / K))\n",
    "        \n",
    "        #initialize accumulation variables\n",
    "        CV_test_MSE = 0\n",
    "        CV_train_MSE = 0\n",
    "\n",
    "        for i in range(len(z)):\n",
    "            \n",
    "            test_start = i * fold_size\n",
    "            test_end = min((i + 1) * fold_size, len(z))\n",
    "            start_stop = npf.arange(test_start, test_end, 1)\n",
    "            X_test_fold = X[start_stop]\n",
    "            y_test_fold = z[start_stop]\n",
    "            rest = npf.setdiff1d(npf.arange(z.shape[0]), start_stop)\n",
    "            X_train_fold = X[rest]\n",
    "            y_train_fold = z[rest]\n",
    "\n",
    "            train_MSE, test_MSE = function(degree, X_train_fold, X_test_fold, y_train_fold, y_test_fold, alpha)\n",
    "\n",
    "            CV_test_MSE += test_MSE\n",
    "            CV_train_MSE += train_MSE\n",
    "\n",
    "            if test_end == len(z):\n",
    "                break\n",
    "\n",
    "        CV_test_MSE /= K\n",
    "        CV_train_MSE /= K\n",
    "\n",
    "        return CV_train_MSE, CV_test_MSE\n",
    "\n",
    "# RegressionClass definition\n",
    "class RegressionClass:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def FFNN_sigmoid(self, degree, X_train, X_test, t_train, t_test, alpha):\n",
    "        pipeline = Pipeline([('poly', PolynomialFeatures(degree=degree, interaction_only=False, include_bias=False))\n",
    "                            ])\n",
    "        \n",
    "        \n",
    "        X_train = pipeline.fit_transform(X_train)\n",
    "        X_test = pipeline.fit_transform(X_test)\n",
    "        \n",
    "        \n",
    "        #Center the design matrices column wise with X_trains column means\n",
    "        scaler = StandardScaler(with_std=False)\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        input_nodes = X_train.shape[1]\n",
    "        output_nodes = 1\n",
    "        linear_regression = FFNN((input_nodes,64,64,64,64,64, output_nodes),hidden_func=sigmoid, output_func=identity, cost_func=CostOLS, seed=2023)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        scheduler =  Adam(eta=1e-4, rho=0.9, rho2=0.9999)\n",
    "        scores = linear_regression.fit(X_train, t_train, scheduler,lam=alpha)\n",
    "\n",
    "        linear_regression.reset_weights() # reset weights such that previous runs or reruns don't affect the weights\n",
    "\n",
    "        scores = linear_regression.fit(X_train, t_train, scheduler, lam=1e-4, epochs=1000)\n",
    "        predictions_train=linear_regression.predict( X_train)\n",
    "        predictions_test=linear_regression.predict( X_test)\n",
    "        MSE_result=npf.array([mean_squared_error(t_train,predictions_train),mean_squared_error(t_train,predictions_train)])\n",
    "        return MSE_result \n",
    "    \n",
    "\n",
    "    def FFNN_relu(self, degree, X_train, X_test, t_train, t_test, alpha):\n",
    "        pipeline = Pipeline([('poly', PolynomialFeatures(degree=degree, interaction_only=False, include_bias=False))\n",
    "                            ])\n",
    "        \n",
    "        \n",
    "        X_train = pipeline.fit_transform(X_train)\n",
    "        X_test = pipeline.fit_transform(X_test)\n",
    "        \n",
    "        \n",
    "        #Center the design matrices column wise with X_trains column means\n",
    "        scaler = StandardScaler(with_std=False)\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        input_nodes = X_train.shape[1]\n",
    "        output_nodes = 1\n",
    "        linear_regression = FFNN((input_nodes,64,64,64,64,64, output_nodes),hidden_func=RELU, output_func=identity, cost_func=CostOLS, seed=2023)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        scheduler =  Adam(eta=1e-10, rho=0.9, rho2=0.9999)\n",
    "        scores = linear_regression.fit(X_train, t_train, scheduler,lam=alpha)\n",
    "\n",
    "        linear_regression.reset_weights() # reset weights such that previous runs or reruns don't affect the weights\n",
    "\n",
    "        scores = linear_regression.fit(X_train, t_train, scheduler, lam=1e-4, epochs=1000)\n",
    "        predictions_train=linear_regression.predict( X_train)\n",
    "        predictions_test=linear_regression.predict( X_test)\n",
    "        MSE_result=npf.array([mean_squared_error(t_train,predictions_train),mean_squared_error(t_train,predictions_train)])\n",
    "        return MSE_result \n",
    "    \n",
    "    def FFNN_lrelu(self, degree, X_train, X_test, t_train, t_test, alpha):\n",
    "        pipeline = Pipeline([('poly', PolynomialFeatures(degree=degree, interaction_only=False, include_bias=False))\n",
    "                            ])\n",
    "        \n",
    "        \n",
    "        X_train = pipeline.fit_transform(X_train)\n",
    "        X_test = pipeline.fit_transform(X_test)\n",
    "        \n",
    "        \n",
    "        #Center the design matrices column wise with X_trains column means\n",
    "        scaler = StandardScaler(with_std=False)\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        input_nodes = X_train.shape[1]\n",
    "        output_nodes = 1\n",
    "        linear_regression = FFNN((input_nodes,64,64,64,64,64, output_nodes),hidden_func=LRELU, output_func=identity, cost_func=CostOLS, seed=2023)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        scheduler =  Adam(eta=1e-10, rho=0.9, rho2=0.9999)\n",
    "        scores = linear_regression.fit(X_train, t_train, scheduler,lam=alpha)\n",
    "\n",
    "        linear_regression.reset_weights() # reset weights such that previous runs or reruns don't affect the weights\n",
    "\n",
    "        scores = linear_regression.fit(X_train, t_train, scheduler, lam=1e-4, epochs=1000)\n",
    "        predictions_train=linear_regression.predict( X_train)\n",
    "        predictions_test=linear_regression.predict( X_test)\n",
    "        MSE_result=npf.array([mean_squared_error(t_train,predictions_train),mean_squared_error(t_train,predictions_train)])\n",
    "        return MSE_result \n",
    "    \n",
    "class RunRegression(RegressionClass, BootstrapKfold):\n",
    "    def __init__(self):\n",
    "        self.regression_functions = {\n",
    "            'FFNN_sigmoid': self.FFNN_sigmoid,\n",
    "            'FFNN_relu': self.FFNN_relu,\n",
    "            'FFNN_lrelu': self.FFNN_lrelu\n",
    "        }\n",
    "\n",
    "    def run(self, regression_type, alpha_values, degrees, X_train, X_test, y_train, y_test, use_bootstrap=False,\n",
    "            use_kfold=False):\n",
    "        \"\"\"\n",
    "        Run regression models with specified parameters.\n",
    "\n",
    "        Parameters:\n",
    "            regression_type (str): Type of regression ('ols', 'ridge', or 'lasso').\n",
    "            alpha_values (list): List of alpha values for Ridge and Lasso regression.\n",
    "            degrees (list): List of polynomial degrees for regression models.\n",
    "            X_train (array): Training features.\n",
    "            X_test (array): Testing features.\n",
    "            y_train (array): Training target values.\n",
    "            y_test (array): Testing target values.\n",
    "            use_bootstrap (bool): Flag to indicate if bootstrap resampling should be performed.\n",
    "            use_kfold (bool): Flag to indicate if K-Fold cross-validation should be performed.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: Dataframe containing regression results.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        #run for all polynomial complexities and tuning paramters defined\n",
    "        for alpha in alpha_values:\n",
    "            alpha_results = []\n",
    "\n",
    "            for degree in degrees:\n",
    "                mse_train_test = self.regression_functions[regression_type](degree, X_train, X_test, y_train, y_test,\n",
    "                                                                            alpha)\n",
    "                alpha_result = {\n",
    "                    'alpha': alpha,\n",
    "                    'degree': degree,\n",
    "                    'mse_train': mse_train_test[0],\n",
    "                    'mse_test': mse_train_test[1],\n",
    "                }\n",
    "                \n",
    "                #include Bootstrap if condition is true\n",
    "                if use_bootstrap:\n",
    "                    cv_train_mse, cv_test_mse = self.bootstrap(100, self.regression_functions[regression_type],\n",
    "                                                               degree, z, X_train, X_test, y_train, y_test, alpha)\n",
    "                    alpha_result['bootstrap_train_mse'] = cv_train_mse\n",
    "                    alpha_result['bootstrap_test_mse'] = cv_test_mse\n",
    "                #Include 5-CV if condition is true\n",
    "                if use_kfold:\n",
    "                    cv_train_mse, cv_test_mse = self.kfold(5, self.regression_functions[regression_type],\n",
    "                                                           degree, z, npf.column_stack((row, column)), alpha)\n",
    "                    alpha_result['5cv_train_mse'] = cv_train_mse\n",
    "                    alpha_result['5cv_test_mse'] = cv_test_mse\n",
    "                 \n",
    "                #Include 10-CV if condition is true\n",
    "            \n",
    "                if use_kfold:\n",
    "                    cv_train_mse, cv_test_mse = self.kfold(10, self.regression_functions[regression_type],\n",
    "                                                           degree, z, npf.column_stack((row, column)), alpha)\n",
    "                    alpha_result['10cv_train_mse'] = cv_train_mse\n",
    "                    alpha_result['10cv_test_mse'] = cv_test_mse\n",
    "\n",
    "                alpha_results.append(alpha_result)\n",
    "\n",
    "            results.extend(alpha_results)\n",
    "\n",
    "        result_df = pd.DataFrame(results)\n",
    "        return result_df\n",
    "    \n",
    "\n",
    "# Create an instance of the RunRegression class\n",
    "regression_instance = RegressionClass()\n",
    "run_regression_instance = RunRegression()\n",
    "\n",
    "# Define the alpha values and degrees\n",
    "alpha_values = [0.00001, 0.00002, 0.00004, 0.0005]\n",
    "degrees = list(range(1,6))\n",
    "\n",
    "# Specify the type of regression ('ols', 'ridge', or 'lasso')\n",
    "regression_type = 'FFNN_sigmoid'  # Change to 'lasso' or 'ols' to run other regressions\n",
    "\n",
    "# Call the run method to perform the specified regression\n",
    "result_df_FFNN_sigmoid = run_regression_instance.run(regression_type, alpha_values,degrees,X_train, X_test, t_train, t_test,use_bootstrap=False, use_kfold=False)\n",
    "\n",
    "\n",
    "result_df_FFNN_sigmoid\n",
    "\n",
    "regression_type = 'FFNN_relu'  # OLS regression\n",
    "\n",
    "# Set alpha to 0 when calling the run method for OLS\n",
    "result_df_FFNN_relu = run_regression_instance.run(regression_type, [0], degrees,  X_train, X_test, t_train, t_test,use_bootstrap=False, use_kfold=False)\n",
    "\n",
    "result_df_FFNN_relu\n",
    "\n",
    "\n",
    "regression_type = 'FFNN_lrelu'  # Lasso regression\n",
    "\n",
    "result_df_FFNN_lrelu = run_regression_instance.run(regression_type,alpha_values, degrees, X_train, X_test, t_train, t_test,use_bootstrap=False, use_kfold=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Assuming you have multiple DataFrames df1, df2, df3\n",
    "display(result_df_FFNN_sigmoid)\n",
    "display(result_df_FFNN_relu)\n",
    "display(result_df_FFNN_lrelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9020506-a319-46c9-81c2-bd104d971f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>degree</th>\n",
       "      <th>mse_train</th>\n",
       "      <th>mse_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>0.034506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031164</td>\n",
       "      <td>0.031164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.090950</td>\n",
       "      <td>0.090950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.063844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>5</td>\n",
       "      <td>0.151807</td>\n",
       "      <td>0.151807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>0.034506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031164</td>\n",
       "      <td>0.031164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>3</td>\n",
       "      <td>0.090950</td>\n",
       "      <td>0.090950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.063844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>5</td>\n",
       "      <td>0.151807</td>\n",
       "      <td>0.151807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>0.034506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031164</td>\n",
       "      <td>0.031164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>3</td>\n",
       "      <td>0.090950</td>\n",
       "      <td>0.090950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.063844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>5</td>\n",
       "      <td>0.151807</td>\n",
       "      <td>0.151807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034506</td>\n",
       "      <td>0.034506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>2</td>\n",
       "      <td>0.031164</td>\n",
       "      <td>0.031164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>3</td>\n",
       "      <td>0.090950</td>\n",
       "      <td>0.090950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063844</td>\n",
       "      <td>0.063844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>5</td>\n",
       "      <td>0.151807</td>\n",
       "      <td>0.151807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha  degree  mse_train  mse_test\n",
       "0   0.00001       1   0.034506  0.034506\n",
       "1   0.00001       2   0.031164  0.031164\n",
       "2   0.00001       3   0.090950  0.090950\n",
       "3   0.00001       4   0.063844  0.063844\n",
       "4   0.00001       5   0.151807  0.151807\n",
       "5   0.00002       1   0.034506  0.034506\n",
       "6   0.00002       2   0.031164  0.031164\n",
       "7   0.00002       3   0.090950  0.090950\n",
       "8   0.00002       4   0.063844  0.063844\n",
       "9   0.00002       5   0.151807  0.151807\n",
       "10  0.00004       1   0.034506  0.034506\n",
       "11  0.00004       2   0.031164  0.031164\n",
       "12  0.00004       3   0.090950  0.090950\n",
       "13  0.00004       4   0.063844  0.063844\n",
       "14  0.00004       5   0.151807  0.151807\n",
       "15  0.00050       1   0.034506  0.034506\n",
       "16  0.00050       2   0.031164  0.031164\n",
       "17  0.00050       3   0.090950  0.090950\n",
       "18  0.00050       4   0.063844  0.063844\n",
       "19  0.00050       5   0.151807  0.151807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>degree</th>\n",
       "      <th>mse_train</th>\n",
       "      <th>mse_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.490930e+07</td>\n",
       "      <td>1.490930e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.452181e+07</td>\n",
       "      <td>2.452181e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.411978e+07</td>\n",
       "      <td>1.411978e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.600550e+08</td>\n",
       "      <td>4.600550e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.824034e+09</td>\n",
       "      <td>1.824034e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  degree     mse_train      mse_test\n",
       "0      0       1  1.490930e+07  1.490930e+07\n",
       "1      0       2  2.452181e+07  2.452181e+07\n",
       "2      0       3  1.411978e+07  1.411978e+07\n",
       "3      0       4  4.600550e+08  4.600550e+08\n",
       "4      0       5  1.824034e+09  1.824034e+09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>degree</th>\n",
       "      <th>mse_train</th>\n",
       "      <th>mse_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>1.483754e+07</td>\n",
       "      <td>1.483754e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>2</td>\n",
       "      <td>2.459653e+07</td>\n",
       "      <td>2.459653e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>3</td>\n",
       "      <td>1.422732e+07</td>\n",
       "      <td>1.422732e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>4</td>\n",
       "      <td>4.584313e+08</td>\n",
       "      <td>4.584313e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>5</td>\n",
       "      <td>1.822756e+09</td>\n",
       "      <td>1.822756e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>1</td>\n",
       "      <td>1.483754e+07</td>\n",
       "      <td>1.483754e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>2</td>\n",
       "      <td>2.459653e+07</td>\n",
       "      <td>2.459653e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>3</td>\n",
       "      <td>1.422732e+07</td>\n",
       "      <td>1.422732e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>4</td>\n",
       "      <td>4.584313e+08</td>\n",
       "      <td>4.584313e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>5</td>\n",
       "      <td>1.822756e+09</td>\n",
       "      <td>1.822756e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>1</td>\n",
       "      <td>1.483754e+07</td>\n",
       "      <td>1.483754e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>2</td>\n",
       "      <td>2.459653e+07</td>\n",
       "      <td>2.459653e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>3</td>\n",
       "      <td>1.422732e+07</td>\n",
       "      <td>1.422732e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>4</td>\n",
       "      <td>4.584313e+08</td>\n",
       "      <td>4.584313e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>5</td>\n",
       "      <td>1.822756e+09</td>\n",
       "      <td>1.822756e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>1</td>\n",
       "      <td>1.483754e+07</td>\n",
       "      <td>1.483754e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>2</td>\n",
       "      <td>2.459653e+07</td>\n",
       "      <td>2.459653e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>3</td>\n",
       "      <td>1.422732e+07</td>\n",
       "      <td>1.422732e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>4</td>\n",
       "      <td>4.584313e+08</td>\n",
       "      <td>4.584313e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>5</td>\n",
       "      <td>1.822756e+09</td>\n",
       "      <td>1.822756e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha  degree     mse_train      mse_test\n",
       "0   0.00001       1  1.483754e+07  1.483754e+07\n",
       "1   0.00001       2  2.459653e+07  2.459653e+07\n",
       "2   0.00001       3  1.422732e+07  1.422732e+07\n",
       "3   0.00001       4  4.584313e+08  4.584313e+08\n",
       "4   0.00001       5  1.822756e+09  1.822756e+09\n",
       "5   0.00002       1  1.483754e+07  1.483754e+07\n",
       "6   0.00002       2  2.459653e+07  2.459653e+07\n",
       "7   0.00002       3  1.422732e+07  1.422732e+07\n",
       "8   0.00002       4  4.584313e+08  4.584313e+08\n",
       "9   0.00002       5  1.822756e+09  1.822756e+09\n",
       "10  0.00004       1  1.483754e+07  1.483754e+07\n",
       "11  0.00004       2  2.459653e+07  2.459653e+07\n",
       "12  0.00004       3  1.422732e+07  1.422732e+07\n",
       "13  0.00004       4  4.584313e+08  4.584313e+08\n",
       "14  0.00004       5  1.822756e+09  1.822756e+09\n",
       "15  0.00050       1  1.483754e+07  1.483754e+07\n",
       "16  0.00050       2  2.459653e+07  2.459653e+07\n",
       "17  0.00050       3  1.422732e+07  1.422732e+07\n",
       "18  0.00050       4  4.584313e+08  4.584313e+08\n",
       "19  0.00050       5  1.822756e+09  1.822756e+09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result_df_FFNN_sigmoid)\n",
    "display(result_df_FFNN_relu)\n",
    "display(result_df_FFNN_lrelu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
